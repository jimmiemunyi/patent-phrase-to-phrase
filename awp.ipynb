{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "import colorlog\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fastcore.xtras import Path  # for ls\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchmetrics import PearsonCorrCoef, MeanSquaredError\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "from composer.loggers import WandBLogger\n",
    "from composer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'apex': True,\n",
    "    'awp_eps': 1e-2,\n",
    "    'awp_lr': 1e-4,\n",
    "    'batch_size': 32, # 2\n",
    "    'batch_scheduler': False,\n",
    "    'betas': (0.9, 0.999),\n",
    "    'ckpt_name': 'deberta_v3_small',\n",
    "    'debug': True, # False\n",
    "    'decoder_lr': 1e-5,\n",
    "    'encoder_lr': 1e-5,\n",
    "    'eps': 1e-6,\n",
    "    'fc_dropout':0.2,\n",
    "    'max_grad_norm': 1000,\n",
    "    'max_len': 400, # 512\n",
    "    'min_lr': 1e-7,\n",
    "    'model_name': 'microsoft/deberta-v3-small',\n",
    "    'n_cycles': 0.5,\n",
    "    'n_epochs': 4, # 12\n",
    "    'n_eval_steps': 100,\n",
    "    'n_folds': 2, # 4\n",
    "    'n_gradient_accumulation_steps': 1,\n",
    "    'n_warmup_steps': 0,\n",
    "    'n_workers': 0,\n",
    "    'nth_awp_start_epoch': 6, # 4\n",
    "    'print_freq': 100,\n",
    "    'scheduler_name': 'cosine',\n",
    "    'seed': 42,\n",
    "    'output_dir': 'output',\n",
    "    'tar_token': '[TAR]',\n",
    "    'wandb': False,\n",
    "    'weight_decay': 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"dataset\")\n",
    "output_path = Path(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, d: dict) -> None:\n",
    "        for k,v in d.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "cfg = Config(d=param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not output_path.exists():\n",
    "    output_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path/\"train.csv\")\n",
    "cpc_titles_df = pd.read_csv(path/\"cpc_titles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score\n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75\n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25\n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50\n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>context_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01</td>\n",
       "      <td>HUMAN NECESSITIES. GRICULTURE; FORESTRY; ANIMA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A21</td>\n",
       "      <td>HUMAN NECESSITIES. BAKING; EDIBLE DOUGHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A22</td>\n",
       "      <td>HUMAN NECESSITIES. BUTCHERING; MEAT TREATMENT;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A23</td>\n",
       "      <td>HUMAN NECESSITIES. FOODS OR FOODSTUFFS; TREATM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A24</td>\n",
       "      <td>HUMAN NECESSITIES. TOBACCO; CIGARS; CIGARETTES...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  context                                       context_text\n",
       "0     A01  HUMAN NECESSITIES. GRICULTURE; FORESTRY; ANIMA...\n",
       "1     A21           HUMAN NECESSITIES. BAKING; EDIBLE DOUGHS\n",
       "2     A22  HUMAN NECESSITIES. BUTCHERING; MEAT TREATMENT;...\n",
       "3     A23  HUMAN NECESSITIES. FOODS OR FOODSTUFFS; TREATM...\n",
       "4     A24  HUMAN NECESSITIES. TOBACCO; CIGARS; CIGARETTES..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpc_titles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.merge(cpc_titles_df, on=\"context\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>context_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score   \n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  \\\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75   \n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25   \n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50   \n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00   \n",
       "\n",
       "                                        context_text  \n",
       "0  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  \n",
       "1  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  \n",
       "2  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  \n",
       "3  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  \n",
       "4  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "special_tokens_dict = {'additional_special_tokens': [cfg.tar_token]}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tar_token_id = tokenizer(f'[{cfg.tar_token}]', add_special_tokens=False)['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "647"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(tokenizer, 'tar_token', f'{cfg.tar_token}')\n",
    "setattr(tokenizer, 'tar_token_id', tar_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(647, '[TAR]')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tar_token_id, tokenizer.tar_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '[SEP]', '[UNK]', '[PAD]', '[MASK]', '[TAR]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['anchor'] + '[SEP]' + train_df['target'] + '[SEP]'  + train_df['context_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.max_len = 133\n",
    "cfg.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>context_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "      <td>abatement[SEP]abatement of pollution[SEP]HUMAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "      <td>abatement[SEP]act of abating[SEP]HUMAN NECESSI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "      <td>abatement[SEP]active catalyst[SEP]HUMAN NECESS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "      <td>abatement[SEP]eliminating process[SEP]HUMAN NE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "      <td>abatement[SEP]forest region[SEP]HUMAN NECESSIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36468</th>\n",
       "      <td>8e1386cbefd7f245</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden article</td>\n",
       "      <td>B44</td>\n",
       "      <td>1.00</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. DECORATIV...</td>\n",
       "      <td>wood article[SEP]wooden article[SEP]PERFORMING...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36469</th>\n",
       "      <td>42d9e032d1cd3242</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden box</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. DECORATIV...</td>\n",
       "      <td>wood article[SEP]wooden box[SEP]PERFORMING OPE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36470</th>\n",
       "      <td>208654ccb9e14fa3</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden handle</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. DECORATIV...</td>\n",
       "      <td>wood article[SEP]wooden handle[SEP]PERFORMING ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36471</th>\n",
       "      <td>756ec035e694722b</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden material</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.75</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. DECORATIV...</td>\n",
       "      <td>wood article[SEP]wooden material[SEP]PERFORMIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36472</th>\n",
       "      <td>8d135da0b55b8c88</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden substrate</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "      <td>PERFORMING OPERATIONS; TRANSPORTING. DECORATIV...</td>\n",
       "      <td>wood article[SEP]wooden substrate[SEP]PERFORMI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36473 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id        anchor                  target context  score   \n",
       "0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50  \\\n",
       "1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75   \n",
       "2      36d72442aefd8232     abatement         active catalyst     A47   0.25   \n",
       "3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50   \n",
       "4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00   \n",
       "...                 ...           ...                     ...     ...    ...   \n",
       "36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00   \n",
       "36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50   \n",
       "36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50   \n",
       "36471  756ec035e694722b  wood article         wooden material     B44   0.75   \n",
       "36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50   \n",
       "\n",
       "                                            context_text   \n",
       "0      HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  \\\n",
       "1      HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n",
       "2      HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n",
       "3      HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n",
       "4      HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...   \n",
       "...                                                  ...   \n",
       "36468  PERFORMING OPERATIONS; TRANSPORTING. DECORATIV...   \n",
       "36469  PERFORMING OPERATIONS; TRANSPORTING. DECORATIV...   \n",
       "36470  PERFORMING OPERATIONS; TRANSPORTING. DECORATIV...   \n",
       "36471  PERFORMING OPERATIONS; TRANSPORTING. DECORATIV...   \n",
       "36472  PERFORMING OPERATIONS; TRANSPORTING. DECORATIV...   \n",
       "\n",
       "                                                    text  \n",
       "0      abatement[SEP]abatement of pollution[SEP]HUMAN...  \n",
       "1      abatement[SEP]act of abating[SEP]HUMAN NECESSI...  \n",
       "2      abatement[SEP]active catalyst[SEP]HUMAN NECESS...  \n",
       "3      abatement[SEP]eliminating process[SEP]HUMAN NE...  \n",
       "4      abatement[SEP]forest region[SEP]HUMAN NECESSIT...  \n",
       "...                                                  ...  \n",
       "36468  wood article[SEP]wooden article[SEP]PERFORMING...  \n",
       "36469  wood article[SEP]wooden box[SEP]PERFORMING OPE...  \n",
       "36470  wood article[SEP]wooden handle[SEP]PERFORMING ...  \n",
       "36471  wood article[SEP]wooden material[SEP]PERFORMIN...  \n",
       "36472  wood article[SEP]wooden substrate[SEP]PERFORMI...  \n",
       "\n",
       "[36473 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer(text,\n",
    "                           add_special_tokens=True,\n",
    "                           max_length=cfg.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df, is_valid=False):\n",
    "        self.cfg = cfg\n",
    "\n",
    "        if is_valid:\n",
    "            _, df = train_test_split(df, test_size=0.2, random_state=cfg.seed)\n",
    "            self.valid_scores = df['score'].explode().to_numpy()\n",
    "        else:\n",
    "            df, _ = train_test_split(df, test_size=0.2, random_state=cfg.seed)\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df['score'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label = torch.tensor(self.labels[item], dtype=torch.float)\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([133])\n",
      "torch.Size([133])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TrainDataset(cfg, train_df)\n",
    "inputs, label = train_dataset[0]\n",
    "print(inputs['input_ids'].shape)\n",
    "print(inputs['attention_mask'].shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([     1,  12462,   4844,      2,  11994,   3036,      2,  97623,   4479,\n",
      "        109320,    346, 112822,   4479,    260,  59248,    430,   3078,  43799,\n",
      "             2,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n",
      "tensor(0.2500)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = TrainDataset(cfg, train_df, is_valid=True)\n",
    "inputs, label = val_dataset[1]\n",
    "print(inputs)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29178, 7295)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 133])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# create training dataloader and get one batch\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                            batch_size=cfg.batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=cfg.n_workers,\n",
    "                            pin_memory=True,\n",
    "                            drop_last=True)\n",
    "\n",
    "inputs, label = next(iter(train_loader))\n",
    "\n",
    "print(inputs['input_ids'].shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "class CustomModel(Module):\n",
    "    def __init__(self, model_name: str, n_vocabs: int) -> None:\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.model_config = AutoConfig.from_pretrained(\n",
    "            model_name, output_hidden_states=True)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name, config=self.model_config)\n",
    "        self.model.resize_token_embeddings(n_vocabs)\n",
    "        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n",
    "        self.fc = nn.Linear(self.model_config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.model_config.hidden_size, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self._init_weights(self.attention)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        # feature = torch.mean(last_hidden_states, 1)\n",
    "        weights = self.attention(last_hidden_states)\n",
    "        feature = torch.sum(weights * last_hidden_states, dim=1)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class AWP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "        criterion: _Loss,\n",
    "        optimizer: Optimizer,\n",
    "        apex: bool,\n",
    "        adv_param: str=\"weight\",\n",
    "        adv_lr: float=1.0,\n",
    "        adv_eps: float=0.01\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.apex = apex\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "\n",
    "    def attack_backward(self, inputs: dict, label: Tensor) -> Tensor:\n",
    "        with torch.cuda.amp.autocast(enabled=self.apex):\n",
    "            self._save()\n",
    "            self._attack_step()\n",
    "            y_preds = self.model(inputs)\n",
    "            adv_loss = self.criterion(\n",
    "                y_preds.view(-1, 1), label.view(-1, 1))\n",
    "            mask = (label.view(-1, 1) != -1)\n",
    "            adv_loss = torch.masked_select(adv_loss, mask).mean()\n",
    "            self.optimizer.zero_grad()\n",
    "        return adv_loss\n",
    "\n",
    "    def _attack_step(self) -> None:\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(\n",
    "                            param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "\n",
    "    def _save(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "import math\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "import scipy as sp\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, awp, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    tot_loss = 0\n",
    "    if not epoch < cfg.nth_awp_start_epoch:\n",
    "        print(f'AWP training with epoch {epoch+1}')\n",
    "    for step, (inputs, labels) in enumerate(train_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "    \n",
    "        if cfg.n_gradient_accumulation_steps > 1:\n",
    "            loss = loss / cfg.n_gradient_accumulation_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "        \n",
    "        # awp stuff\n",
    "        if cfg.nth_awp_start_epoch <= epoch:\n",
    "            loss = awp.attack_backward(inputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            awp._restore()\n",
    "        \n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        tot_loss += loss.item()\n",
    "        end = time.time()\n",
    "        if (step + 1) % cfg.n_gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if cfg.batch_scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "        if step % cfg.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                'Elapsed {remain:s} '\n",
    "                'Loss: {loss.val:.4f}({avg_loss:.4f}) '\n",
    "                'Grad: {grad_norm:.4f}  '\n",
    "                'LR: {lr:.8f}  '\n",
    "                .format(epoch+1, step, len(train_loader), \n",
    "                        remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                        loss=losses,\n",
    "                        avg_loss=tot_loss/(step+1),\n",
    "                        grad_norm=grad_norm,\n",
    "                        lr=scheduler.get_lr()[0]))\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, labels) in enumerate(valid_loader):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        if cfg.n_gradient_accumulation_steps > 1:\n",
    "            loss = loss / cfg.n_gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    predictions = np.concatenate(predictions)\n",
    "    return losses.avg, predictions\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "\n",
    "def get_score(y_true: ndarray, y_pred: ndarray) -> float:\n",
    "    score = sp.stats.pearsonr(y_true, y_pred)[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import gc\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(train_df, awp=False):\n",
    "    if awp:\n",
    "        cfg.nth_awp_start_epoch = 1\n",
    "    train_dataset = TrainDataset(cfg, train_df)\n",
    "    valid_dataset = TrainDataset(cfg, train_df, is_valid=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=cfg.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=cfg.n_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=cfg.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=cfg.n_workers, pin_memory=True, drop_last=False)\n",
    "    valid_labels = valid_dataset.valid_scores\n",
    "\n",
    "    # ====================================================\n",
    "    #  tokenzier & model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(cfg.model_name, n_vocabs=len(cfg.tokenizer))\n",
    "    # torch.save(model.model_config, f'{cfg.output_dir}config.pth')\n",
    "    model.cuda()\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=cfg.encoder_lr, \n",
    "                                                decoder_lr=cfg.decoder_lr,\n",
    "                                                weight_decay=cfg.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=cfg.encoder_lr, eps=cfg.eps, betas=cfg.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler_name == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler_name == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.n_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.n_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_dataset) // cfg.batch_size * cfg.n_epochs)\n",
    "    scheduler = get_scheduler(cfg, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "    # ====================================================\n",
    "    # awp\n",
    "    # ====================================================\n",
    "    awp = AWP(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        apex=cfg.apex,\n",
    "        adv_lr=cfg.awp_lr,\n",
    "        adv_eps=cfg.awp_eps,\n",
    "    )\n",
    "    \n",
    "    best_score = 0.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    for epoch in range(cfg.n_epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, model, awp, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        # scoring\n",
    "        score = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        print(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "        if cfg.wandb:\n",
    "            wandb.log({f\"epoch\": epoch+1, \n",
    "                       f\"avg_train_loss\": avg_loss, \n",
    "                       f\"avg_val_loss\": avg_val_loss,\n",
    "                       f\"score\": score})\n",
    "        \n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            # torch.save({'model': model.state_dict(),\n",
    "            #             'predictions': predictions},\n",
    "            #             cfg.output_dir+f\"{cfg.model.replace('/', '-')}_best.pth\")\n",
    "        \n",
    "        if not cfg.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    # predictions = torch.load(cfg.output_dir+f\"{cfg.model.replace('/', '-')}_best.pth\", \n",
    "    #                          map_location=torch.device('cpu'))['predictions']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/911] Elapsed 0m 1s (remain 21m 54s) Loss: 0.8752(0.8752) Grad: 545037.3125  LR: 0.00001000  \n",
      "Epoch: [1][100/911] Elapsed 0m 33s (remain 4m 32s) Loss: 0.6190(0.6654) Grad: 40512.6719  LR: 0.00001000  \n",
      "Epoch: [1][200/911] Elapsed 1m 6s (remain 3m 55s) Loss: 0.5877(0.6424) Grad: 54439.4766  LR: 0.00001000  \n",
      "Epoch: [1][300/911] Elapsed 1m 39s (remain 3m 21s) Loss: 0.6557(0.6283) Grad: 103767.4219  LR: 0.00001000  \n",
      "Epoch: [1][400/911] Elapsed 2m 12s (remain 2m 48s) Loss: 0.5348(0.6186) Grad: 79035.4141  LR: 0.00001000  \n",
      "Epoch: [1][500/911] Elapsed 2m 45s (remain 2m 15s) Loss: 0.5374(0.6126) Grad: 123632.1016  LR: 0.00001000  \n",
      "Epoch: [1][600/911] Elapsed 3m 18s (remain 1m 42s) Loss: 0.5955(0.6078) Grad: 94763.8125  LR: 0.00001000  \n",
      "Epoch: [1][700/911] Elapsed 3m 51s (remain 1m 9s) Loss: 0.5996(0.6032) Grad: 112590.0781  LR: 0.00001000  \n",
      "Epoch: [1][800/911] Elapsed 4m 23s (remain 0m 36s) Loss: 0.6070(0.5987) Grad: 191803.1406  LR: 0.00001000  \n",
      "Epoch: [1][900/911] Elapsed 4m 56s (remain 0m 3s) Loss: 0.5741(0.5957) Grad: 121624.8203  LR: 0.00001000  \n",
      "Epoch: [1][910/911] Elapsed 5m 0s (remain 0m 0s) Loss: 0.5068(0.5953) Grad: 174429.5469  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 22s) Loss: 0.5684(0.5684) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.5371(0.5749) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5508(0.5762) \n",
      "EVAL: [227/228] Elapsed 0m 22s (remain 0m 0s) Loss: 0.4839(0.5748) \n",
      "Epoch 1 - avg_train_loss: 0.5953  avg_val_loss: 0.5748  time: 323s\n",
      "Epoch 1 - Score: 0.7649\n",
      "Epoch 1 - Save Best Score: 0.7649 Model\n",
      "Epoch: [2][0/911] Elapsed 0m 0s (remain 4m 36s) Loss: 0.5204(0.5204) Grad: 94423.0938  LR: 0.00001000  \n",
      "Epoch: [2][100/911] Elapsed 0m 33s (remain 4m 26s) Loss: 0.5742(0.5636) Grad: 146482.0625  LR: 0.00001000  \n",
      "Epoch: [2][200/911] Elapsed 1m 6s (remain 3m 53s) Loss: 0.5688(0.5628) Grad: 96700.4922  LR: 0.00001000  \n",
      "Epoch: [2][300/911] Elapsed 1m 39s (remain 3m 20s) Loss: 0.5503(0.5636) Grad: 83938.0156  LR: 0.00001000  \n",
      "Epoch: [2][400/911] Elapsed 2m 12s (remain 2m 47s) Loss: 0.5876(0.5625) Grad: 67360.9688  LR: 0.00001000  \n",
      "Epoch: [2][500/911] Elapsed 2m 45s (remain 2m 15s) Loss: 0.5078(0.5605) Grad: 85925.4375  LR: 0.00001000  \n",
      "Epoch: [2][600/911] Elapsed 3m 17s (remain 1m 42s) Loss: 0.5816(0.5595) Grad: 186045.1250  LR: 0.00001000  \n",
      "Epoch: [2][700/911] Elapsed 3m 50s (remain 1m 9s) Loss: 0.5210(0.5593) Grad: 83100.4531  LR: 0.00001000  \n",
      "Epoch: [2][800/911] Elapsed 4m 23s (remain 0m 36s) Loss: 0.4971(0.5583) Grad: 122368.3594  LR: 0.00001000  \n",
      "Epoch: [2][900/911] Elapsed 4m 56s (remain 0m 3s) Loss: 0.5699(0.5582) Grad: 77673.8125  LR: 0.00001000  \n",
      "Epoch: [2][910/911] Elapsed 5m 0s (remain 0m 0s) Loss: 0.5487(0.5579) Grad: 120145.3828  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 22s) Loss: 0.5531(0.5531) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.4896(0.5482) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5167(0.5499) \n",
      "EVAL: [227/228] Elapsed 0m 22s (remain 0m 0s) Loss: 0.4447(0.5481) \n",
      "Epoch 2 - avg_train_loss: 0.5579  avg_val_loss: 0.5481  time: 323s\n",
      "Epoch 2 - Score: 0.8063\n",
      "Epoch 2 - Save Best Score: 0.8063 Model\n",
      "Epoch: [3][0/911] Elapsed 0m 0s (remain 4m 36s) Loss: 0.4797(0.4797) Grad: 73237.1953  LR: 0.00001000  \n",
      "Epoch: [3][100/911] Elapsed 0m 33s (remain 4m 26s) Loss: 0.5816(0.5454) Grad: 81553.7969  LR: 0.00001000  \n",
      "Epoch: [3][200/911] Elapsed 1m 6s (remain 3m 53s) Loss: 0.5891(0.5442) Grad: 93796.7266  LR: 0.00001000  \n",
      "Epoch: [3][300/911] Elapsed 1m 39s (remain 3m 21s) Loss: 0.5057(0.5438) Grad: 109292.1953  LR: 0.00001000  \n",
      "Epoch: [3][400/911] Elapsed 2m 12s (remain 2m 48s) Loss: 0.6156(0.5457) Grad: 57589.5508  LR: 0.00001000  \n",
      "Epoch: [3][500/911] Elapsed 2m 45s (remain 2m 15s) Loss: 0.5068(0.5449) Grad: 92294.7266  LR: 0.00001000  \n",
      "Epoch: [3][600/911] Elapsed 3m 18s (remain 1m 42s) Loss: 0.5125(0.5446) Grad: 52450.4375  LR: 0.00001000  \n",
      "Epoch: [3][700/911] Elapsed 3m 51s (remain 1m 9s) Loss: 0.5696(0.5449) Grad: 110079.9766  LR: 0.00001000  \n",
      "Epoch: [3][800/911] Elapsed 4m 23s (remain 0m 36s) Loss: 0.5741(0.5440) Grad: 206600.5469  LR: 0.00001000  \n",
      "Epoch: [3][900/911] Elapsed 4m 56s (remain 0m 3s) Loss: 0.5097(0.5440) Grad: 92761.1016  LR: 0.00001000  \n",
      "Epoch: [3][910/911] Elapsed 5m 0s (remain 0m 0s) Loss: 0.5967(0.5441) Grad: 114420.6953  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 22s) Loss: 0.5603(0.5603) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.4844(0.5430) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5248(0.5433) \n",
      "EVAL: [227/228] Elapsed 0m 22s (remain 0m 0s) Loss: 0.4389(0.5415) \n",
      "Epoch 3 - avg_train_loss: 0.5441  avg_val_loss: 0.5415  time: 323s\n",
      "Epoch 3 - Score: 0.8223\n",
      "Epoch 3 - Save Best Score: 0.8223 Model\n",
      "Epoch: [4][0/911] Elapsed 0m 0s (remain 4m 35s) Loss: 0.5646(0.5646) Grad: 157695.6719  LR: 0.00001000  \n",
      "Epoch: [4][100/911] Elapsed 0m 33s (remain 4m 26s) Loss: 0.5956(0.5417) Grad: 170708.1562  LR: 0.00001000  \n",
      "Epoch: [4][200/911] Elapsed 1m 6s (remain 3m 54s) Loss: 0.5623(0.5381) Grad: 134330.1875  LR: 0.00001000  \n",
      "Epoch: [4][300/911] Elapsed 1m 39s (remain 3m 21s) Loss: 0.5340(0.5350) Grad: 80618.4922  LR: 0.00001000  \n",
      "Epoch: [4][400/911] Elapsed 2m 12s (remain 2m 48s) Loss: 0.5192(0.5366) Grad: 102202.9062  LR: 0.00001000  \n",
      "Epoch: [4][500/911] Elapsed 2m 45s (remain 2m 15s) Loss: 0.5908(0.5355) Grad: 163474.2344  LR: 0.00001000  \n",
      "Epoch: [4][600/911] Elapsed 3m 18s (remain 1m 42s) Loss: 0.5520(0.5349) Grad: 84158.0781  LR: 0.00001000  \n",
      "Epoch: [4][700/911] Elapsed 3m 51s (remain 1m 9s) Loss: 0.4695(0.5353) Grad: 119987.5156  LR: 0.00001000  \n",
      "Epoch: [4][800/911] Elapsed 4m 24s (remain 0m 36s) Loss: 0.4957(0.5344) Grad: 109649.9844  LR: 0.00001000  \n",
      "Epoch: [4][900/911] Elapsed 4m 56s (remain 0m 3s) Loss: 0.5120(0.5354) Grad: 119800.7031  LR: 0.00001000  \n",
      "Epoch: [4][910/911] Elapsed 5m 0s (remain 0m 0s) Loss: 0.5171(0.5353) Grad: 147664.5156  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 22s) Loss: 0.5613(0.5613) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.4755(0.5495) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5189(0.5494) \n",
      "EVAL: [227/228] Elapsed 0m 22s (remain 0m 0s) Loss: 0.4392(0.5473) \n",
      "Epoch 4 - avg_train_loss: 0.5353  avg_val_loss: 0.5473  time: 323s\n",
      "Epoch 4 - Score: 0.8249\n",
      "Epoch 4 - Save Best Score: 0.8249 Model\n"
     ]
    }
   ],
   "source": [
    "# without awp\n",
    "train_loop(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/911] Elapsed 0m 0s (remain 4m 38s) Loss: 0.6674(0.6674) Grad: 161547.9062  LR: 0.00001000  \n",
      "Epoch: [1][100/911] Elapsed 0m 33s (remain 4m 25s) Loss: 0.6023(0.6613) Grad: 208309.6250  LR: 0.00001000  \n",
      "Epoch: [1][200/911] Elapsed 1m 5s (remain 3m 52s) Loss: 0.6154(0.6494) Grad: 55361.1289  LR: 0.00001000  \n",
      "Epoch: [1][300/911] Elapsed 1m 38s (remain 3m 20s) Loss: 0.5816(0.6342) Grad: 71358.9062  LR: 0.00001000  \n",
      "Epoch: [1][400/911] Elapsed 2m 11s (remain 2m 47s) Loss: 0.5448(0.6262) Grad: 181526.3125  LR: 0.00001000  \n",
      "Epoch: [1][500/911] Elapsed 2m 44s (remain 2m 14s) Loss: 0.5799(0.6174) Grad: 76826.3203  LR: 0.00001000  \n",
      "Epoch: [1][600/911] Elapsed 3m 17s (remain 1m 41s) Loss: 0.5512(0.6101) Grad: 156022.9219  LR: 0.00001000  \n",
      "Epoch: [1][700/911] Elapsed 3m 50s (remain 1m 9s) Loss: 0.6321(0.6044) Grad: 88221.1953  LR: 0.00001000  \n",
      "Epoch: [1][800/911] Elapsed 4m 23s (remain 0m 36s) Loss: 0.5716(0.6004) Grad: 151570.7500  LR: 0.00001000  \n",
      "Epoch: [1][900/911] Elapsed 4m 56s (remain 0m 3s) Loss: 0.6578(0.5982) Grad: 149756.2812  LR: 0.00001000  \n",
      "Epoch: [1][910/911] Elapsed 4m 59s (remain 0m 0s) Loss: 0.5245(0.5979) Grad: 92585.8906  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 22s) Loss: 0.5623(0.5623) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.5201(0.5567) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5309(0.5584) \n",
      "EVAL: [227/228] Elapsed 0m 22s (remain 0m 0s) Loss: 0.4596(0.5572) \n",
      "Epoch 1 - avg_train_loss: 0.5979  avg_val_loss: 0.5572  time: 323s\n",
      "Epoch 1 - Score: 0.7810\n",
      "Epoch 1 - Save Best Score: 0.7810 Model\n",
      "AWP training with epoch 2\n",
      "Epoch: [2][0/911] Elapsed 0m 0s (remain 7m 31s) Loss: 0.6387(0.6387) Grad: 77611.3281  LR: 0.00001000  \n",
      "Epoch: [2][100/911] Elapsed 0m 52s (remain 6m 58s) Loss: 0.6080(0.5933) Grad: 40205.5039  LR: 0.00001000  \n",
      "Epoch: [2][200/911] Elapsed 1m 43s (remain 6m 6s) Loss: 0.5612(0.5816) Grad: 36352.8984  LR: 0.00001000  \n",
      "Epoch: [2][300/911] Elapsed 2m 35s (remain 5m 15s) Loss: 0.6006(0.5820) Grad: 36424.8828  LR: 0.00001000  \n",
      "Epoch: [2][400/911] Elapsed 3m 27s (remain 4m 23s) Loss: 0.5960(0.5800) Grad: 42759.6367  LR: 0.00001000  \n",
      "Epoch: [2][500/911] Elapsed 4m 18s (remain 3m 31s) Loss: 0.5950(0.5799) Grad: 26276.7734  LR: 0.00001000  \n",
      "Epoch: [2][600/911] Elapsed 5m 10s (remain 2m 40s) Loss: 0.6727(0.5777) Grad: 38511.3203  LR: 0.00001000  \n",
      "Epoch: [2][700/911] Elapsed 6m 2s (remain 1m 48s) Loss: 0.5992(0.5760) Grad: 36498.7031  LR: 0.00001000  \n",
      "Epoch: [2][800/911] Elapsed 6m 54s (remain 0m 56s) Loss: 0.5522(0.5750) Grad: 20174.1895  LR: 0.00001000  \n",
      "Epoch: [2][900/911] Elapsed 7m 45s (remain 0m 5s) Loss: 0.5388(0.5738) Grad: 41929.6367  LR: 0.00001000  \n",
      "Epoch: [2][910/911] Elapsed 7m 50s (remain 0m 0s) Loss: 0.5340(0.5734) Grad: 29693.0605  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 22s) Loss: 0.5545(0.5545) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.4968(0.5438) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5191(0.5449) \n",
      "EVAL: [227/228] Elapsed 0m 22s (remain 0m 0s) Loss: 0.4547(0.5438) \n",
      "Epoch 2 - avg_train_loss: 0.5734  avg_val_loss: 0.5438  time: 494s\n",
      "Epoch 2 - Score: 0.8264\n",
      "Epoch 2 - Save Best Score: 0.8264 Model\n",
      "AWP training with epoch 3\n",
      "Epoch: [3][0/911] Elapsed 0m 0s (remain 7m 28s) Loss: 0.5288(0.5288) Grad: 66423.1250  LR: 0.00001000  \n",
      "Epoch: [3][100/911] Elapsed 0m 52s (remain 6m 58s) Loss: 0.5818(0.5529) Grad: 66507.3203  LR: 0.00001000  \n",
      "Epoch: [3][200/911] Elapsed 1m 43s (remain 6m 6s) Loss: 0.4081(0.5497) Grad: 63849.4922  LR: 0.00001000  \n",
      "Epoch: [3][300/911] Elapsed 2m 34s (remain 5m 14s) Loss: 0.5894(0.5509) Grad: 76739.1484  LR: 0.00001000  \n",
      "Epoch: [3][400/911] Elapsed 3m 26s (remain 4m 22s) Loss: 0.5997(0.5503) Grad: 50602.4492  LR: 0.00001000  \n",
      "Epoch: [3][500/911] Elapsed 4m 17s (remain 3m 31s) Loss: 0.5957(0.5501) Grad: 41898.3203  LR: 0.00001000  \n",
      "Epoch: [3][600/911] Elapsed 5m 9s (remain 2m 39s) Loss: 0.6013(0.5503) Grad: 57687.1836  LR: 0.00001000  \n",
      "Epoch: [3][700/911] Elapsed 6m 1s (remain 1m 48s) Loss: 0.5810(0.5501) Grad: 41750.2109  LR: 0.00001000  \n",
      "Epoch: [3][800/911] Elapsed 6m 52s (remain 0m 56s) Loss: 0.5143(0.5498) Grad: 47854.2422  LR: 0.00001000  \n",
      "Epoch: [3][900/911] Elapsed 7m 44s (remain 0m 5s) Loss: 0.5439(0.5502) Grad: 46476.8594  LR: 0.00001000  \n",
      "Epoch: [3][910/911] Elapsed 7m 49s (remain 0m 0s) Loss: 0.5226(0.5501) Grad: 40771.5391  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 22s) Loss: 0.5477(0.5477) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.4793(0.5365) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5135(0.5377) \n",
      "EVAL: [227/228] Elapsed 0m 22s (remain 0m 0s) Loss: 0.4387(0.5363) \n",
      "Epoch 3 - avg_train_loss: 0.5501  avg_val_loss: 0.5363  time: 493s\n",
      "Epoch 3 - Score: 0.8377\n",
      "Epoch 3 - Save Best Score: 0.8377 Model\n",
      "AWP training with epoch 4\n",
      "Epoch: [4][0/911] Elapsed 0m 0s (remain 7m 36s) Loss: 0.4766(0.4766) Grad: 63590.1914  LR: 0.00001000  \n",
      "Epoch: [4][100/911] Elapsed 0m 52s (remain 6m 58s) Loss: 0.5620(0.5368) Grad: 55111.2461  LR: 0.00001000  \n",
      "Epoch: [4][200/911] Elapsed 1m 43s (remain 6m 6s) Loss: 0.5716(0.5389) Grad: 115408.6641  LR: 0.00001000  \n",
      "Epoch: [4][300/911] Elapsed 2m 35s (remain 5m 15s) Loss: 0.5694(0.5384) Grad: 49721.5938  LR: 0.00001000  \n",
      "Epoch: [4][400/911] Elapsed 3m 26s (remain 4m 23s) Loss: 0.4655(0.5371) Grad: 28040.5156  LR: 0.00001000  \n",
      "Epoch: [4][500/911] Elapsed 4m 18s (remain 3m 31s) Loss: 0.5651(0.5368) Grad: 57000.8516  LR: 0.00001000  \n",
      "Epoch: [4][600/911] Elapsed 5m 9s (remain 2m 39s) Loss: 0.5792(0.5376) Grad: 34094.6055  LR: 0.00001000  \n",
      "Epoch: [4][700/911] Elapsed 6m 1s (remain 1m 48s) Loss: 0.6144(0.5376) Grad: 58035.5039  LR: 0.00001000  \n",
      "Epoch: [4][800/911] Elapsed 6m 53s (remain 0m 56s) Loss: 0.5230(0.5391) Grad: 39611.1211  LR: 0.00001000  \n",
      "Epoch: [4][900/911] Elapsed 7m 45s (remain 0m 5s) Loss: 0.6161(0.5389) Grad: 78726.2578  LR: 0.00001000  \n",
      "Epoch: [4][910/911] Elapsed 7m 50s (remain 0m 0s) Loss: 0.4694(0.5388) Grad: 37812.2617  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 23s) Loss: 0.5470(0.5470) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.4706(0.5375) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5127(0.5377) \n",
      "EVAL: [227/228] Elapsed 0m 23s (remain 0m 0s) Loss: 0.4376(0.5360) \n",
      "Epoch 4 - avg_train_loss: 0.5388  avg_val_loss: 0.5360  time: 493s\n",
      "Epoch 4 - Score: 0.8443\n",
      "Epoch 4 - Save Best Score: 0.8443 Model\n"
     ]
    }
   ],
   "source": [
    "# with awp\n",
    "train_loop(train_df, awp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/911] Elapsed 0m 0s (remain 4m 37s) Loss: 0.7636(0.7636) Grad: 509024.6250  LR: 0.00001000  \n",
      "Epoch: [1][100/911] Elapsed 0m 32s (remain 4m 21s) Loss: 0.6684(0.6632) Grad: 56533.6484  LR: 0.00001000  \n",
      "Epoch: [1][200/911] Elapsed 1m 4s (remain 3m 49s) Loss: 0.6121(0.6444) Grad: 82577.4766  LR: 0.00001000  \n",
      "Epoch: [1][300/911] Elapsed 1m 37s (remain 3m 17s) Loss: 0.6381(0.6294) Grad: 98416.9062  LR: 0.00001000  \n",
      "Epoch: [1][400/911] Elapsed 2m 10s (remain 2m 45s) Loss: 0.6236(0.6204) Grad: 97415.6719  LR: 0.00001000  \n",
      "Epoch: [1][500/911] Elapsed 2m 42s (remain 2m 13s) Loss: 0.6316(0.6118) Grad: 126235.6016  LR: 0.00001000  \n",
      "Epoch: [1][600/911] Elapsed 3m 15s (remain 1m 40s) Loss: 0.5452(0.6052) Grad: 106304.9766  LR: 0.00001000  \n",
      "Epoch: [1][700/911] Elapsed 3m 48s (remain 1m 8s) Loss: 0.5044(0.6008) Grad: 95169.6250  LR: 0.00001000  \n",
      "Epoch: [1][800/911] Elapsed 4m 21s (remain 0m 35s) Loss: 0.5103(0.5976) Grad: 92704.4688  LR: 0.00001000  \n",
      "Epoch: [1][900/911] Elapsed 4m 54s (remain 0m 3s) Loss: 0.6041(0.5943) Grad: 104061.8281  LR: 0.00001000  \n",
      "Epoch: [1][910/911] Elapsed 4m 57s (remain 0m 0s) Loss: 0.5440(0.5936) Grad: 89777.9609  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 22s) Loss: 0.5723(0.5723) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.5247(0.5625) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5374(0.5644) \n",
      "EVAL: [227/228] Elapsed 0m 22s (remain 0m 0s) Loss: 0.4716(0.5633) \n",
      "Epoch 1 - avg_train_loss: 0.5936  avg_val_loss: 0.5633  time: 320s\n",
      "Epoch 1 - Score: 0.7781\n",
      "Epoch 1 - Save Best Score: 0.7781 Model\n",
      "AWP training with epoch 2\n",
      "Epoch: [2][0/911] Elapsed 0m 0s (remain 7m 27s) Loss: 0.5512(0.5512) Grad: 76479.0234  LR: 0.00001000  \n",
      "Epoch: [2][100/911] Elapsed 0m 52s (remain 6m 57s) Loss: 0.5453(0.5922) Grad: 34027.2500  LR: 0.00001000  \n",
      "Epoch: [2][200/911] Elapsed 1m 43s (remain 6m 6s) Loss: 0.6703(0.5821) Grad: 84812.0000  LR: 0.00001000  \n",
      "Epoch: [2][300/911] Elapsed 2m 35s (remain 5m 14s) Loss: 0.5831(0.5802) Grad: 48874.0703  LR: 0.00001000  \n",
      "Epoch: [2][400/911] Elapsed 3m 26s (remain 4m 23s) Loss: 0.5813(0.5780) Grad: 32511.5391  LR: 0.00001000  \n",
      "Epoch: [2][500/911] Elapsed 4m 18s (remain 3m 31s) Loss: 0.5955(0.5761) Grad: 37840.7891  LR: 0.00001000  \n",
      "Epoch: [2][600/911] Elapsed 5m 10s (remain 2m 40s) Loss: 0.5186(0.5752) Grad: 25014.5469  LR: 0.00001000  \n",
      "Epoch: [2][700/911] Elapsed 6m 1s (remain 1m 48s) Loss: 0.6104(0.5752) Grad: 31409.2598  LR: 0.00001000  \n",
      "Epoch: [2][800/911] Elapsed 6m 53s (remain 0m 56s) Loss: 0.4916(0.5747) Grad: 28531.0449  LR: 0.00001000  \n",
      "Epoch: [2][900/911] Elapsed 7m 45s (remain 0m 5s) Loss: 0.5625(0.5740) Grad: 21040.2812  LR: 0.00001000  \n",
      "Epoch: [2][910/911] Elapsed 7m 50s (remain 0m 0s) Loss: 0.5668(0.5739) Grad: 25524.2090  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 22s) Loss: 0.5504(0.5504) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.4972(0.5432) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5192(0.5441) \n",
      "EVAL: [227/228] Elapsed 0m 22s (remain 0m 0s) Loss: 0.4503(0.5429) \n",
      "Epoch 2 - avg_train_loss: 0.5739  avg_val_loss: 0.5429  time: 493s\n",
      "Epoch 2 - Score: 0.8239\n",
      "Epoch 2 - Save Best Score: 0.8239 Model\n",
      "AWP training with epoch 3\n",
      "Epoch: [3][0/911] Elapsed 0m 0s (remain 7m 30s) Loss: 0.5731(0.5731) Grad: 49139.7969  LR: 0.00001000  \n",
      "Epoch: [3][100/911] Elapsed 0m 52s (remain 6m 58s) Loss: 0.6280(0.5528) Grad: 42714.9805  LR: 0.00001000  \n",
      "Epoch: [3][200/911] Elapsed 1m 43s (remain 6m 6s) Loss: 0.5432(0.5479) Grad: 59697.7500  LR: 0.00001000  \n",
      "Epoch: [3][300/911] Elapsed 2m 35s (remain 5m 15s) Loss: 0.5553(0.5491) Grad: 58952.1172  LR: 0.00001000  \n",
      "Epoch: [3][400/911] Elapsed 3m 27s (remain 4m 23s) Loss: 0.5081(0.5499) Grad: 69979.1875  LR: 0.00001000  \n",
      "Epoch: [3][500/911] Elapsed 4m 18s (remain 3m 31s) Loss: 0.5258(0.5509) Grad: 39054.9297  LR: 0.00001000  \n",
      "Epoch: [3][600/911] Elapsed 5m 10s (remain 2m 40s) Loss: 0.5438(0.5508) Grad: 43710.9414  LR: 0.00001000  \n",
      "Epoch: [3][700/911] Elapsed 6m 2s (remain 1m 48s) Loss: 0.5495(0.5508) Grad: 44739.2383  LR: 0.00001000  \n",
      "Epoch: [3][800/911] Elapsed 6m 53s (remain 0m 56s) Loss: 0.5380(0.5512) Grad: 62592.0039  LR: 0.00001000  \n",
      "Epoch: [3][900/911] Elapsed 7m 45s (remain 0m 5s) Loss: 0.6207(0.5510) Grad: 189809.4844  LR: 0.00001000  \n",
      "Epoch: [3][910/911] Elapsed 7m 50s (remain 0m 0s) Loss: 0.5407(0.5510) Grad: 64951.5859  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 22s) Loss: 0.5431(0.5431) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.4753(0.5379) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5132(0.5387) \n",
      "EVAL: [227/228] Elapsed 0m 22s (remain 0m 0s) Loss: 0.4414(0.5371) \n",
      "Epoch 3 - avg_train_loss: 0.5510  avg_val_loss: 0.5371  time: 494s\n",
      "Epoch 3 - Score: 0.8373\n",
      "Epoch 3 - Save Best Score: 0.8373 Model\n",
      "AWP training with epoch 4\n",
      "Epoch: [4][0/911] Elapsed 0m 0s (remain 7m 27s) Loss: 0.5308(0.5308) Grad: 74524.7812  LR: 0.00001000  \n",
      "Epoch: [4][100/911] Elapsed 0m 52s (remain 6m 58s) Loss: 0.6324(0.5413) Grad: 67877.6953  LR: 0.00001000  \n",
      "Epoch: [4][200/911] Elapsed 1m 43s (remain 6m 6s) Loss: 0.5085(0.5409) Grad: 44464.5625  LR: 0.00001000  \n",
      "Epoch: [4][300/911] Elapsed 2m 35s (remain 5m 15s) Loss: 0.4882(0.5402) Grad: 52787.4609  LR: 0.00001000  \n",
      "Epoch: [4][400/911] Elapsed 3m 27s (remain 4m 23s) Loss: 0.5573(0.5390) Grad: 77300.1719  LR: 0.00001000  \n",
      "Epoch: [4][500/911] Elapsed 4m 18s (remain 3m 31s) Loss: 0.4385(0.5391) Grad: 39065.7305  LR: 0.00001000  \n",
      "Epoch: [4][600/911] Elapsed 5m 10s (remain 2m 40s) Loss: 0.5945(0.5396) Grad: 29650.1074  LR: 0.00001000  \n",
      "Epoch: [4][700/911] Elapsed 6m 2s (remain 1m 48s) Loss: 0.5738(0.5403) Grad: 44588.5508  LR: 0.00001000  \n",
      "Epoch: [4][800/911] Elapsed 6m 53s (remain 0m 56s) Loss: 0.5875(0.5399) Grad: 60000.1523  LR: 0.00001000  \n",
      "Epoch: [4][900/911] Elapsed 7m 45s (remain 0m 5s) Loss: 0.5076(0.5399) Grad: 60789.2383  LR: 0.00001000  \n",
      "Epoch: [4][910/911] Elapsed 7m 50s (remain 0m 0s) Loss: 0.5583(0.5399) Grad: 47661.2773  LR: 0.00001000  \n",
      "EVAL: [0/228] Elapsed 0m 0s (remain 0m 22s) Loss: 0.5460(0.5460) \n",
      "EVAL: [100/228] Elapsed 0m 10s (remain 0m 12s) Loss: 0.4762(0.5355) \n",
      "EVAL: [200/228] Elapsed 0m 20s (remain 0m 2s) Loss: 0.5188(0.5356) \n",
      "EVAL: [227/228] Elapsed 0m 22s (remain 0m 0s) Loss: 0.4371(0.5341) \n",
      "Epoch 4 - avg_train_loss: 0.5399  avg_val_loss: 0.5341  time: 493s\n",
      "Epoch 4 - Score: 0.8436\n",
      "Epoch 4 - Save Best Score: 0.8436 Model\n"
     ]
    }
   ],
   "source": [
    "# awp experimental\n",
    "train_loop(train_df, awp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
