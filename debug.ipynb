{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fastcore.xtras import Path  # for ls\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "from torchmetrics import PearsonCorrCoef\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "from composer import Trainer\n",
    "from composer.metrics import CrossEntropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignoring warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def process_df(df: pd.DataFrame, train: bool = True):\n",
    "    df[\"input\"] = (\n",
    "        \"TEXT1: \" + df.context + \"; TEXT2: \" + df.target + \"; ANC1: \" + df.anchor\n",
    "    )\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    if train:\n",
    "        dataset = dataset.rename_columns({\"score\": \"labels\"})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "path = Path(\"dataset\")\n",
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "test_df = pd.read_csv(path / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36468</th>\n",
       "      <td>8e1386cbefd7f245</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden article</td>\n",
       "      <td>B44</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36469</th>\n",
       "      <td>42d9e032d1cd3242</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden box</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36470</th>\n",
       "      <td>208654ccb9e14fa3</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden handle</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36471</th>\n",
       "      <td>756ec035e694722b</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden material</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36472</th>\n",
       "      <td>8d135da0b55b8c88</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden substrate</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36473 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id        anchor                  target context  score\n",
       "0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50\n",
       "1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75\n",
       "2      36d72442aefd8232     abatement         active catalyst     A47   0.25\n",
       "3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50\n",
       "4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00\n",
       "...                 ...           ...                     ...     ...    ...\n",
       "36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00\n",
       "36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50\n",
       "36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50\n",
       "36471  756ec035e694722b  wood article         wooden material     B44   0.75\n",
       "36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50\n",
       "\n",
       "[36473 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'anchor', 'target', 'context', 'labels', 'input'],\n",
      "    num_rows: 36473\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_ds = process_df(train_df)\n",
    "eval_ds = process_df(test_df, train=False)\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '37d61fd2272659b1',\n",
       " 'anchor': 'abatement',\n",
       " 'target': 'abatement of pollution',\n",
       " 'context': 'A47',\n",
       " 'labels': 0.5,\n",
       " 'input': 'TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/deberta-v3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(batch, tokenizer=tokenizer):\n",
    "    return tokenizer(\n",
    "        batch[\"input\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 54453,   435,   294,   336,  5753,   346, 54453,   445,   294,\n",
       "         47284,   265,  6435,   346, 23702,   435,   294, 47284,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_func(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ade4f74c3574ade9f5cb6e94a5bb00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd77c44fa8da4b4daf2fe07ba8f036ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tok_ds = train_ds.map(tokenize_func, batched=True, batch_size=None)\n",
    "eval_tok_ds = eval_ds.map(tokenize_func, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 36473\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset\n",
    "train_dds = train_tok_ds.train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 29178\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 7295\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the training dataset lengths are similar\n",
      "57\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking if the training dataset lengths are similar\")\n",
    "print(len(train_dds[\"train\"][0][\"input_ids\"]))\n",
    "print(len(train_dds[\"train\"][1][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the test dataset lengths are similar\n",
      "57\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking if the test dataset lengths are similar\")\n",
    "print(len(train_dds[\"test\"][0][\"input_ids\"]))\n",
    "print(len(train_dds[\"test\"][1][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating PyTorch dataloaders\n",
    "train_dl = DataLoader(\n",
    "    train_dds[\"train\"],\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    train_dds[\"test\"],\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    collate_fn=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.utils.data.dataloader.DataLoader,\n",
       " torch.utils.data.dataloader.DataLoader)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dl), type(val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch\n",
      "tensor([    1, 54453,   435,   294,   336,  5718,   346, 54453,   445,   294,\n",
      "         4823,   346, 23702,   435,   294,  6624,  3480,     2,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor(0.2500)\n"
     ]
    }
   ],
   "source": [
    "# get a sample batch and print the first element\n",
    "print(\"Sample batch\")\n",
    "batch = next(iter(val_dl))\n",
    "print(batch[\"input_ids\"][0])\n",
    "print(batch[\"token_type_ids\"][0])\n",
    "print(batch[\"attention_mask\"][0])\n",
    "print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([0.2500, 0.2500, 0.0000, 0.5000, 0.5000, 0.2500, 0.2500, 0.5000, 0.0000,\n",
       "         0.2500, 0.0000, 0.2500, 0.2500, 0.0000, 0.0000, 0.5000, 0.2500, 0.2500,\n",
       "         0.2500, 0.5000, 0.5000, 0.5000, 0.2500, 0.5000, 0.2500, 0.2500, 0.5000,\n",
       "         0.7500, 0.2500, 0.0000, 1.0000, 0.5000, 0.2500, 0.2500, 0.0000, 0.5000,\n",
       "         0.5000, 0.5000, 0.0000, 0.2500, 0.2500, 0.0000, 0.2500, 0.7500, 0.5000,\n",
       "         0.5000, 0.0000, 0.2500, 0.5000, 0.5000, 0.0000, 0.5000, 0.0000, 0.0000,\n",
       "         0.0000, 0.2500, 0.2500, 1.0000, 0.2500, 0.5000, 0.2500, 0.7500, 0.2500,\n",
       "         0.7500]),\n",
       " 'input_ids': tensor([[    1, 54453,   435,  ...,     0,     0,     0],\n",
       "         [    1, 54453,   435,  ...,     0,     0,     0],\n",
       "         [    1, 54453,   435,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    1, 54453,   435,  ...,     0,     0,     0],\n",
       "         [    1, 54453,   435,  ...,     0,     0,     0],\n",
       "         [    1, 54453,   435,  ...,     0,     0,     0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pears_corr = PearsonCorrCoef(num_outputs=1)\n",
    "composer_model = HuggingFaceModel(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    metrics=[CrossEntropy()],\n",
    "    eval_metrics=[CrossEntropy(), pears_corr],\n",
    "    use_logits=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    params=composer_model.parameters(),\n",
    "    lr=8e-5,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-6,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_lr_decay = LinearLR(\n",
    "    optimizer, start_factor=1.0, end_factor=0, total_iters=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=composer_model,\n",
    "    train_dataloader=train_dl,\n",
    "    eval_dataloader=val_dl,\n",
    "    max_duration=\"1ep\",\n",
    "    optimizers=optimizer,\n",
    "    schedulers=[linear_lr_decay],\n",
    "    device=\"gpu\",\n",
    "    precision=\"amp_fp16\",\n",
    "    # seed=17,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 trainer.fit()                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">66</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fit</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1763 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.scaler = ClosureGradScaler() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._use_closures() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> GradSca  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1764 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1765 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.first_batch_complete = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1766 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_loop()                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1767 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1768 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">close</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1769 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"Shutdown the trainer.</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">19</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">40</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_train_loop</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1937 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.logger.log_metrics({<span style=\"color: #808000; text-decoration-color: #808000\">'time/token'</span>: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.timestamp.toke  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1938 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.logger.log_metrics({<span style=\"color: #808000; text-decoration-color: #808000\">'time/token_in_epoch'</span>: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.times  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1939 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1940 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>total_loss_dict = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch(use_grad_scaling)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1941 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1942 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> use_grad_scaling:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1943 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.scaler.update()                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">21</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">18</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_train_batch</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   </span>optimizer.step(closure=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">lambda</span> **kwargs: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_microbat  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   │   </span>microbatches, total_loss_dict, **kwargs).item())          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2118 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_microbatches(microbatches, total_loss_dict)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2119 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed_enabled:                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2120 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> optimizer <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.optimizers:                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2121 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> use_grad_scaling:                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">22</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">13</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_train_microbatches</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2210 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2211 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> microbatch_idx, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.batch <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(microbatches):              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2212 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>is_final_microbatch = microbatch_idx + <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> == <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(microbatches)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2213 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>microbatch_loss_dict = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_microbatch(use_grad_scaling, current_  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2214 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2215 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Aggregate each loss in microbatch_loss_dict into total_loss_dict</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2216 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> k, microbatch_loss <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> microbatch_loss_dict.items():                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">47</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_train_microbatch</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2344 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Use microbatch outputs to update training metrics</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2345 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.train_metrics <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2346 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.train_metrics = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._ensure_metrics_device_and_dtype(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.st  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2347 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._eval_train_metrics(device_batch)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2348 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2349 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed_enabled:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2350 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.deepspeed_model.step()                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">57</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_eval_train_metrics</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2054 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad(),\\                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2055 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>model_eval_mode(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.model),\\                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2056 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>_get_precision_context(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.precision, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.deepspeed_enabled):     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2057 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>eval_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._original_model.eval_forward(device_batch, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.ou  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2058 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> _, metric <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.train_metrics.items():                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2059 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._original_model.update_metric(                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2060 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>device_batch,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/models/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">huggingface.py</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> :<span style=\"color: #0000ff; text-decoration-color: #0000ff\">375</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">eval_forward</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">372 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = output[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(output[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>].shape) == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> output[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">373 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">374 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># if we are in the single class case, then remove the classes dimension</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>375 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> output.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">376 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = output.squeeze(dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">377 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">378 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = outputs <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> outputs <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.forward(batch)                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">IndexError: </span>tuple index out of range\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 trainer.fit()                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m17\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m66\u001b[0m in \u001b[92mfit\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1763 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.state.scaler = ClosureGradScaler() \u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._use_closures() \u001b[94melse\u001b[0m GradSca  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1764 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1765 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.first_batch_complete = \u001b[94mFalse\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1766 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._train_loop()                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1767 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1768 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mclose\u001b[0m(\u001b[96mself\u001b[0m):                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1769 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33m\"\"\"Shutdown the trainer.\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m19\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m40\u001b[0m in \u001b[92m_train_loop\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1937 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.logger.log_metrics({\u001b[33m'\u001b[0m\u001b[33mtime/token\u001b[0m\u001b[33m'\u001b[0m: \u001b[96mself\u001b[0m.state.timestamp.toke  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1938 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.logger.log_metrics({\u001b[33m'\u001b[0m\u001b[33mtime/token_in_epoch\u001b[0m\u001b[33m'\u001b[0m: \u001b[96mself\u001b[0m.state.times  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1939 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1940 \u001b[2m│   │   │   │   │   \u001b[0mtotal_loss_dict = \u001b[96mself\u001b[0m._train_batch(use_grad_scaling)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1941 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1942 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m use_grad_scaling:                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1943 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.state.scaler.update()                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m21\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m18\u001b[0m in \u001b[92m_train_batch\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2115 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0moptimizer.step(closure=\u001b[94mlambda\u001b[0m **kwargs: \u001b[96mself\u001b[0m._train_microbat  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2116 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   \u001b[0mmicrobatches, total_loss_dict, **kwargs).item())          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2117 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2118 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._train_microbatches(microbatches, total_loss_dict)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2119 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m.deepspeed_enabled:                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2120 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m optimizer \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.state.optimizers:                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2121 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[94mif\u001b[0m use_grad_scaling:                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m22\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m13\u001b[0m in \u001b[92m_train_microbatches\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2210 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2211 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m microbatch_idx, \u001b[96mself\u001b[0m.state.batch \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(microbatches):              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2212 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mis_final_microbatch = microbatch_idx + \u001b[94m1\u001b[0m == \u001b[96mlen\u001b[0m(microbatches)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2213 \u001b[2m│   │   │   │   \u001b[0mmicrobatch_loss_dict = \u001b[96mself\u001b[0m._train_microbatch(use_grad_scaling, current_  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2214 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2215 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Aggregate each loss in microbatch_loss_dict into total_loss_dict\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2216 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mfor\u001b[0m k, microbatch_loss \u001b[95min\u001b[0m microbatch_loss_dict.items():                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m23\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m47\u001b[0m in \u001b[92m_train_microbatch\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2344 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Use microbatch outputs to update training metrics\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2345 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.state.train_metrics \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2346 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.state.train_metrics = \u001b[96mself\u001b[0m._ensure_metrics_device_and_dtype(\u001b[96mself\u001b[0m.st  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2347 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._eval_train_metrics(device_batch)                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2348 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2349 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.deepspeed_enabled:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2350 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.state.deepspeed_model.step()                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/trainer/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m20\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m57\u001b[0m in \u001b[92m_eval_train_metrics\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2054 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad(),\\                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2055 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodel_eval_mode(\u001b[96mself\u001b[0m.state.model),\\                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2056 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m_get_precision_context(\u001b[96mself\u001b[0m.state.precision, \u001b[96mself\u001b[0m.deepspeed_enabled):     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2057 \u001b[2m│   │   │   \u001b[0meval_outputs = \u001b[96mself\u001b[0m._original_model.eval_forward(device_batch, \u001b[96mself\u001b[0m.state.ou  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2058 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m _, metric \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.state.train_metrics.items():                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2059 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._original_model.update_metric(                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2060 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mdevice_batch,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/eleven/mambaforge/envs/nlpbook/lib/python3.10/site-packages/composer/models/\u001b[0m\u001b[1;33mhuggingface.py\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m :\u001b[94m375\u001b[0m in \u001b[92meval_forward\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m372 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = output[\u001b[94m1\u001b[0m] \u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(output[\u001b[94m0\u001b[0m].shape) == \u001b[94m0\u001b[0m \u001b[94melse\u001b[0m output[\u001b[94m0\u001b[0m]             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m373 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m374 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# if we are in the single class case, then remove the classes dimension\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m375 \u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m output.shape[\u001b[94m1\u001b[0m] == \u001b[94m1\u001b[0m:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m376 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = output.squeeze(dim=\u001b[94m1\u001b[0m)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m377 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m378 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput = outputs \u001b[94mif\u001b[0m outputs \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.forward(batch)                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mIndexError: \u001b[0mtuple index out of range\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
