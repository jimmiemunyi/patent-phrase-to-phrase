{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import colorlog\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fastcore.xtras import Path  # for ls\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchmetrics import PearsonCorrCoef, MeanSquaredError\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "from composer.loggers import WandBLogger\n",
    "from composer import Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignoring warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def process_df(df, sep_token):\n",
    "    df[\"section\"] = df.context.str[0]\n",
    "    df[\"sectok\"] = \"[\" + df.section + \"]\"\n",
    "    sectoks = list(df.sectok.unique())\n",
    "    df[\"input\"] = (\n",
    "        df.sectok\n",
    "        + sep_token\n",
    "        + df.context\n",
    "        + sep_token\n",
    "        + df.anchor.str.lower()\n",
    "        + sep_token\n",
    "        + df.target\n",
    "    )\n",
    "    \n",
    "    return df, sectoks\n",
    "\n",
    "\n",
    "def create_val_split(df: pd.DataFrame, val_prop: float = 0.2, seed: int = 42):\n",
    "    anchors = df.anchor.unique()\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(anchors)\n",
    "    val_sz = int(len(anchors) * val_prop)\n",
    "    val_anchors = anchors[:val_sz]\n",
    "    is_val = np.isin(df.anchor, val_anchors)\n",
    "    idxs = np.arange(len(df))\n",
    "    val_idxs = idxs[is_val]\n",
    "    trn_idxs = idxs[~is_val]\n",
    "\n",
    "    return trn_idxs, val_idxs\n",
    "\n",
    "def tokenize_func(batch, tokenizer):\n",
    "    return tokenizer(\n",
    "        batch[\"input\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize_and_split(df, tokenize_func, train=True):\n",
    "    inps = \"anchor\", \"target\", \"context\"\n",
    "    dataset = datasets.Dataset.from_pandas(df)\n",
    "    tok_dataset = dataset.map(\n",
    "        tokenize_func,\n",
    "        batched=True,\n",
    "        batch_size=None,\n",
    "        remove_columns=inps + (\"id\", \"input\", \"section\", \"sectok\")\n",
    "    )\n",
    "    if train:\n",
    "        tok_dataset = tok_dataset.rename_columns({\"score\": \"labels\"})\n",
    "        trn_idxs, val_idxs = create_val_split(df)\n",
    "        tok_dataset = datasets.DatasetDict(\n",
    "        {\"train\": tok_dataset.select(trn_idxs), \"test\": tok_dataset.select(val_idxs)}\n",
    "    )\n",
    "    \n",
    "    return tok_dataset\n",
    "\n",
    "\n",
    "def create_dataloaders(tok_ds, bs, train=True):\n",
    "    if train:\n",
    "        train_dl = DataLoader(\n",
    "            tok_ds[\"train\"],\n",
    "            batch_size=bs,\n",
    "            shuffle=True,\n",
    "            collate_fn=default_data_collator,\n",
    "        )\n",
    "        val_dl = DataLoader(\n",
    "            tok_ds[\"test\"],\n",
    "            batch_size=bs,\n",
    "            shuffle=False,\n",
    "            collate_fn=default_data_collator,\n",
    "        )\n",
    "\n",
    "        return train_dl, val_dl\n",
    "    else:\n",
    "        test_dl = DataLoader(\n",
    "            tok_ds,\n",
    "            batch_size=bs,\n",
    "            shuffle=False,\n",
    "            collate_fn=default_data_collator,\n",
    "        )\n",
    "\n",
    "        return test_dl\n",
    "\n",
    "\n",
    "def predict(trainer, test_dl):\n",
    "    preds = trainer.predict(test_dl)[0][\"logits\"].numpy().astype(float)\n",
    "    preds = np.clip(preds, 0, 1)\n",
    "    preds = preds.round(2)\n",
    "    preds = preds.squeeze()\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df, tokenizer, sep_token, bs):\n",
    "    train_df, sectoks = process_df(train_df, sep_token)\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": sectoks})\n",
    "    tokenize = partial(tokenize_func, tokenizer=tokenizer)\n",
    "    train_tok_ds = tokenize_and_split(train_df, tokenize)\n",
    "    train_dl, val_dl = create_dataloaders(train_tok_ds, bs)\n",
    "    \n",
    "    return train_dl, val_dl\n",
    "\n",
    "def prepare_model(checkpoint, num_labels, tokenizer):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint, num_labels=num_labels\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    pears_corr = PearsonCorrCoef(num_outputs=num_labels)\n",
    "    mse_metric = MeanSquaredError()\n",
    "    composer_model = HuggingFaceModel(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        metrics=[pears_corr],\n",
    "        eval_metrics=[mse_metric, pears_corr],\n",
    "        use_logits=True,\n",
    "    )\n",
    "    \n",
    "    return composer_model\n",
    "\n",
    "def prepare_optimizer_and_scheduler(composer_model, lr, wd, epochs, train_dl):\n",
    "    optimizer = AdamW(\n",
    "        params=composer_model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.98),\n",
    "        eps=1e-6,\n",
    "        weight_decay=wd,\n",
    "    )\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        steps_per_epoch=len(train_dl),\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler\n",
    "\n",
    "def prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs, run_name):\n",
    "    trainer = Trainer(\n",
    "        model=composer_model,\n",
    "        train_dataloader=train_dl,\n",
    "        eval_dataloader=val_dl,\n",
    "        max_duration=f\"{epochs}ep\",\n",
    "        optimizers=optimizer,\n",
    "        schedulers=[scheduler],\n",
    "        loggers=[WandBLogger(project=\"patent-phrase-to-phrase\")],\n",
    "        run_name=run_name,\n",
    "        device=\"gpu\",\n",
    "        precision=\"amp_fp16\",\n",
    "        step_schedulers_every_batch=True,\n",
    "        # seed=17,\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_df, checkpoint, run_name, bs=32, lr=8e-5, wd=0.01, epochs=4, num_labels=1, sep_token=\" [s] \"):\n",
    "    # preparing data\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    train_dl, val_dl = prepare_data(train_df, tokenizer, sep_token, bs)\n",
    "    \n",
    "    # preparing model\n",
    "    composer_model = prepare_model(checkpoint, num_labels, tokenizer)\n",
    "    \n",
    "    # preparing optimizer and scheduler\n",
    "    optimizer, scheduler = prepare_optimizer_and_scheduler(composer_model, lr, wd, epochs, train_dl)\n",
    "    \n",
    "    # preparing trainer\n",
    "    trainer = prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs, run_name)\n",
    "    \n",
    "    # training\n",
    "    trainer.fit()\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "test_df = pd.read_csv(path / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token = \" [s] \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, sectoks = process_df(train_df, sep_token)\n",
    "eval_df, _ = process_df(test_df, sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/deberta-v3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\"additional_special_tokens\": sectoks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = partial(tokenize_func, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok_ds = tokenize_and_split(train_df, tokenize)\n",
    "eval_tok_ds = tokenize_and_split(eval_df, tokenize, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5\n",
    "bs = 64\n",
    "epochs = 4\n",
    "num_labels =1\n",
    "wd = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl = create_dataloaders(train_tok_ds, bs)\n",
    "test_dl = create_dataloaders(eval_tok_ds, bs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample batch and print the first element\n",
    "print(\"Sample batch\")\n",
    "batch = next(iter(val_dl))\n",
    "print(batch[\"input_ids\"][0])\n",
    "print(batch[\"token_type_ids\"][0])\n",
    "print(batch[\"attention_mask\"][0])\n",
    "print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"token_type_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pears_corr = PearsonCorrCoef(num_outputs=num_labels)\n",
    "mse_metric = MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_model = HuggingFaceModel(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    metrics=[pears_corr],\n",
    "    eval_metrics=[mse_metric, pears_corr],\n",
    "    use_logits=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    params=composer_model.parameters(),\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-6,\n",
    "    weight_decay=wd,\n",
    ")\n",
    "\n",
    "one_cycle_lr = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=lr,\n",
    "    steps_per_epoch=len(train_dl),\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "trainer = train(train_df, checkpoint, run_name=\"baseline\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation Zone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Different Sep Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5\n",
    "bs = 64\n",
    "epochs = 4\n",
    "num_labels =1\n",
    "wd = 0.01\n",
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "# sep_token = \" [s] \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "train_dl, val_dl = prepare_data(train_df, tokenizer, sep_token=tokenizer.sep_token, bs=bs)\n",
    "composer_model = prepare_model(checkpoint, num_labels, tokenizer)\n",
    "optimizer, scheduler = prepare_optimizer_and_scheduler(composer_model, lr, wd, epochs, train_dl)\n",
    "trainer = prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs, run_name=\"tok_sep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.eval_metric_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Classification task instead of Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_to_class = {\n",
    "    0: 0,\n",
    "    0.25: 1,\n",
    "    0.5: 2,\n",
    "    0.75: 3,\n",
    "    1: 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the function to the dataframe\n",
    "train_df[\"score\"] = train_df[\"score\"].apply(lambda x: score_to_class[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5\n",
    "bs = 64\n",
    "epochs = 4\n",
    "num_labels = 5\n",
    "wd = 0.01\n",
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "sep_token = \" [s] \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "from composer.metrics import CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_classification(checkpoint, num_labels, tokenizer):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint, num_labels=num_labels\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    pears_corr = PearsonCorrCoef(num_outputs=num_labels)\n",
    "    cross_entropy = CrossEntropy()\n",
    "    accuracy_metric = Accuracy(task='multiclass', num_classes=num_labels)\n",
    "    composer_model = HuggingFaceModel(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        metrics=[cross_entropy, accuracy_metric],\n",
    "        # eval_metrics=[mse_metric, pears_corr],\n",
    "        use_logits=True,\n",
    "    )\n",
    "    \n",
    "    return composer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "train_dl, val_dl = prepare_data(train_df, tokenizer, sep_token=sep_token, bs=bs)\n",
    "composer_model = prepare_model_classification(checkpoint, num_labels, tokenizer)\n",
    "optimizer, scheduler = prepare_optimizer_and_scheduler(composer_model, lr, wd, epochs, train_dl)\n",
    "trainer = prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.eval_metric_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could not get the main metric (Pearson Correlation) to work in a classification setting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Different Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "checkpoint = \"anferico/bert-for-patents\"\n",
    "trainer = train(train_df, checkpoint, lr=8e-6, run_name=\"bert-for-patents\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "checkpoint = \"AI-Growth-Lab/PatentSBERTa\"\n",
    "trainer = train(train_df, checkpoint, run_name=\"PatentSBERTa\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Cosine Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.optim import CosineAnnealingWithWarmupScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_optimizer_and_scheduler(composer_model, lr, wd, epochs, train_dl):\n",
    "    optimizer = AdamW(\n",
    "        params=composer_model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.98),\n",
    "        eps=1e-6,\n",
    "        weight_decay=wd,\n",
    "    )\n",
    "    scheduler = CosineAnnealingWithWarmupScheduler(\n",
    "        t_warmup='0.2dur'\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "trainer = train(train_df, checkpoint, run_name=\"cosine_scheduler\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Replacing the Context with the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score\n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75\n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25\n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50\n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               37d61fd2272659b1\n",
       "anchor                  abatement\n",
       "target     abatement of pollution\n",
       "context                       A47\n",
       "score                         0.5\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_row = train_df.iloc[0]\n",
    "sample_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_csv(path / \"titles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>group</th>\n",
       "      <th>main_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>HUMAN NECESSITIES</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01</td>\n",
       "      <td>AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTI...</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01B</td>\n",
       "      <td>SOIL WORKING IN AGRICULTURE OR FORESTRY; PARTS...</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01B1/00</td>\n",
       "      <td>Hand tools (edge trimmers for lawns A01G3/06  ...</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01B1/02</td>\n",
       "      <td>Spades; Shovels {(hand-operated dredgers E02F3...</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       code                                              title section  class   \n",
       "0         A                                  HUMAN NECESSITIES       A    NaN  \\\n",
       "1       A01  AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTI...       A    1.0   \n",
       "2      A01B  SOIL WORKING IN AGRICULTURE OR FORESTRY; PARTS...       A    1.0   \n",
       "3  A01B1/00  Hand tools (edge trimmers for lawns A01G3/06  ...       A    1.0   \n",
       "4  A01B1/02  Spades; Shovels {(hand-operated dredgers E02F3...       A    1.0   \n",
       "\n",
       "  subclass  group  main_group  \n",
       "0      NaN    NaN         NaN  \n",
       "1      NaN    NaN         NaN  \n",
       "2        B    NaN         NaN  \n",
       "3        B    1.0         0.0  \n",
       "4        B    1.0         2.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; COFFEE MILLS; SPICE MILLS; SUCTION CLEANERS IN GENERAL'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.loc[titles[\"code\"] == sample_row[\"context\"]].title.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes matching the context to the code\n",
    "train_df = train_df.merge(titles, left_on=\"context\", right_on=\"code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>code</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>group</th>\n",
       "      <th>main_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score code   \n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  A47  \\\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  A47   \n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25  A47   \n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50  A47   \n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00  A47   \n",
       "\n",
       "                                               title section  class subclass   \n",
       "0  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN  \\\n",
       "1  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "2  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "3  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "4  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "\n",
       "   group  main_group  \n",
       "0    NaN         NaN  \n",
       "1    NaN         NaN  \n",
       "2    NaN         NaN  \n",
       "3    NaN         NaN  \n",
       "4    NaN         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the title instead of the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, sep_token):\n",
    "    df[\"section\"] = df.context.str[0]\n",
    "    df[\"sectok\"] = \"[\" + df.section + \"]\"\n",
    "    sectoks = list(df.sectok.unique())\n",
    "    df[\"input\"] = (\n",
    "        df.sectok\n",
    "        + sep_token\n",
    "        # + \"context: \"\n",
    "        + df.title.str.lower()\n",
    "        + sep_token\n",
    "        + df.anchor.str.lower()\n",
    "        + sep_token\n",
    "        + df.target\n",
    "    )\n",
    "    \n",
    "    return df, sectoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy, _ = process_df(train_df, \" [s] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[A] [s] furniture; domestic articles or appliances; coffee mills; spice mills; suction cleaners in general [s] abatement [s] abatement of pollution'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.iloc[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[A] [s] furniture; domestic articles or appliances; coffee mills; spice mills; suction cleaners in general [s] cervical support [s] gel pack'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.iloc[100].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>code</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>group</th>\n",
       "      <th>main_group</th>\n",
       "      <th>sectok</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[A] [s] furniture; domestic articles or applia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[A] [s] furniture; domestic articles or applia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[A] [s] furniture; domestic articles or applia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[A] [s] furniture; domestic articles or applia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[A] [s] furniture; domestic articles or applia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score code   \n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  A47  \\\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  A47   \n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25  A47   \n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50  A47   \n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00  A47   \n",
       "\n",
       "                                               title section  class subclass   \n",
       "0  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN  \\\n",
       "1  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "2  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "3  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "4  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "\n",
       "   group  main_group sectok                                              input  \n",
       "0    NaN         NaN    [A]  [A] [s] furniture; domestic articles or applia...  \n",
       "1    NaN         NaN    [A]  [A] [s] furniture; domestic articles or applia...  \n",
       "2    NaN         NaN    [A]  [A] [s] furniture; domestic articles or applia...  \n",
       "3    NaN         NaN    [A]  [A] [s] furniture; domestic articles or applia...  \n",
       "4    NaN         NaN    [A]  [A] [s] furniture; domestic articles or applia...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd99f3cd1994b3481a2553bfbc061ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjimmiemunyi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/eleven/code/personal/patent-phrase-to-phrase/wandb/run-20230620_044435-z0jxe85e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/z0jxe85e' target=\"_blank\">context_title</a></strong> to <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/z0jxe85e' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/z0jxe85e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "node_name: ubuntu_desktop\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 456369393\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8872476a6f42738714edc28bb2a905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1bca121d354b10933b1705da99dfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   0:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a1ea769c2640e793e977ef7de46adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   1:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dafbbceca314f0285472deec5667ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   1:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19c6169cb064f17b8e521e9d0e355e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   2:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a554befbc0d74518b93aeab6c5478929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   2:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842842eaf36a48d78d18f7ad8c99bdba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   3:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2017ecfb7e8340cca016d18e1bd8ce7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   3:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MeanSquaredError': tensor(0.0268, device='cuda:0'), 'PearsonCorrCoef': tensor(0.7982, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "trainer = train(train_df, checkpoint, run_name=\"context_title\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some improvements! Potential is there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td></td></tr><tr><td>metrics/eval/MeanSquaredError</td><td></td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td></td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td></td></tr><tr><td>time/batch</td><td></td></tr><tr><td>time/batch_in_epoch</td><td></td></tr><tr><td>time/epoch</td><td></td></tr><tr><td>time/sample</td><td></td></tr><tr><td>time/sample_in_epoch</td><td></td></tr><tr><td>time/token</td><td></td></tr><tr><td>time/token_in_epoch</td><td></td></tr><tr><td>trainer/device_train_microbatch_size</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>0.00601</td></tr><tr><td>metrics/eval/MeanSquaredError</td><td>0.02683</td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td>0.79822</td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td>0.94622</td></tr><tr><td>time/batch</td><td>3624</td></tr><tr><td>time/batch_in_epoch</td><td>0</td></tr><tr><td>time/epoch</td><td>4</td></tr><tr><td>time/sample</td><td>115868</td></tr><tr><td>time/sample_in_epoch</td><td>0</td></tr><tr><td>time/token</td><td>8689575</td></tr><tr><td>time/token_in_epoch</td><td>2172000</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>32</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">context_title</strong> at: <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/z0jxe85e' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/z0jxe85e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230620_044435-z0jxe85e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Different arrangements of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, sep_token):\n",
    "    df[\"section\"] = df.context.str[0]\n",
    "    df[\"sectok\"] = \"[\" + df.section + \"]\"\n",
    "    sectoks = list(df.sectok.unique())\n",
    "    df[\"input\"] = (\n",
    "        df.anchor.str.lower()\n",
    "        + sep_token\n",
    "        + df.target\n",
    "        + sep_token\n",
    "        + df.title.str.lower()\n",
    "    )\n",
    "    # df[\"input\"] = (\n",
    "    #     df.sectok\n",
    "    #     + sep_token\n",
    "    #     # + \"context: \"\n",
    "    #     + df.title.str.lower()\n",
    "    #     + sep_token\n",
    "    #     + df.anchor.str.lower()\n",
    "    #     + sep_token\n",
    "    #     + df.target\n",
    "    # )\n",
    "    \n",
    "    return df, sectoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "titles = pd.read_csv(path / \"titles.csv\")\n",
    "train_df = train_df.merge(titles, left_on=\"context\", right_on=\"code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy, _ = process_df(train_df, \" [s] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abatement [s] abatement of pollution [s] furniture; domestic articles or appliances; coffee mills; spice mills; suction cleaners in general'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.iloc[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e46ec97ec0440895bbde3ddb03546c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/eleven/code/personal/patent-phrase-to-phrase/wandb/run-20230620_051446-i4w4z8iy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/i4w4z8iy' target=\"_blank\">anchor_target_title</a></strong> to <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/i4w4z8iy' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/i4w4z8iy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "node_name: ubuntu_desktop\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 2979262394\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c33f8365654ba0af37530a0d9b5cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0962eba891424ef9badf96b282fc9413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   0:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845f837818624230be0fa224ed4bdd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   1:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0179039bc77d4b86b1bd9285304635b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   1:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933a1f3d9d9f4330ba37c6d02333f268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   2:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c08976aaf049cebb9283a1e8eb534d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   2:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e9e0e8d6934d00bfa858d2347b0a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   3:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1e445dd84d438db2a04487a4cdc52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   3:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MeanSquaredError': tensor(0.0273, device='cuda:0'), 'PearsonCorrCoef': tensor(0.7924, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "trainer = train(train_df, checkpoint, run_name=\"anchor_target_title\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td></td></tr><tr><td>metrics/eval/MeanSquaredError</td><td></td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td></td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td></td></tr><tr><td>time/batch</td><td></td></tr><tr><td>time/batch_in_epoch</td><td></td></tr><tr><td>time/epoch</td><td></td></tr><tr><td>time/sample</td><td></td></tr><tr><td>time/sample_in_epoch</td><td></td></tr><tr><td>time/token</td><td></td></tr><tr><td>time/token_in_epoch</td><td></td></tr><tr><td>trainer/device_train_microbatch_size</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>0.02186</td></tr><tr><td>metrics/eval/MeanSquaredError</td><td>0.02733</td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td>0.79245</td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td>0.81018</td></tr><tr><td>time/batch</td><td>3624</td></tr><tr><td>time/batch_in_epoch</td><td>0</td></tr><tr><td>time/epoch</td><td>4</td></tr><tr><td>time/sample</td><td>115868</td></tr><tr><td>time/sample_in_epoch</td><td>0</td></tr><tr><td>time/token</td><td>8226131</td></tr><tr><td>time/token_in_epoch</td><td>2056160</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>32</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">anchor_target_title</strong> at: <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/i4w4z8iy' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/i4w4z8iy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230620_051446-i4w4z8iy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) Updates from Kaggle 1st Place Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPC Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "cpc_path = Path('cpc-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('cpc-data/CPCSchemeXML202105'),Path('cpc-data/CPCTitleList202202')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpc_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_cpc_texts():\n",
    "    contexts = []\n",
    "    pattern = '[A-Z]\\d+'\n",
    "    for file_name in (cpc_path/'CPCSchemeXML202105').ls():\n",
    "        result = re.findall(pattern, file_name.name)\n",
    "        if result:\n",
    "            contexts.append(result)\n",
    "    contexts = sorted(set(sum(contexts, [])))\n",
    "    results = {}\n",
    "    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n",
    "        with open(cpc_path/f'CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n",
    "            s = f.read()\n",
    "        pattern = f'{cpc}\\t\\t.+'\n",
    "        result = re.findall(pattern, s)\n",
    "        cpc_result = result[0].lstrip(pattern)\n",
    "        if cpc == 'C':\n",
    "            cpc_result = 'C' + cpc_result\n",
    "        for context in [c for c in contexts if c[0] == cpc]:\n",
    "            pattern = f'{context}\\t\\t.+'\n",
    "            result = re.findall(pattern, s)\n",
    "            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpc_texts = get_cpc_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>context_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01</td>\n",
       "      <td>HUMAN NECESSITIES. GRICULTURE; FORESTRY; ANIMA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A21</td>\n",
       "      <td>HUMAN NECESSITIES. BAKING; EDIBLE DOUGHS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A22</td>\n",
       "      <td>HUMAN NECESSITIES. BUTCHERING; MEAT TREATMENT;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A23</td>\n",
       "      <td>HUMAN NECESSITIES. FOODS OR FOODSTUFFS; TREATM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A24</td>\n",
       "      <td>HUMAN NECESSITIES. TOBACCO; CIGARS; CIGARETTES...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  context                                       context_text\n",
       "0     A01  HUMAN NECESSITIES. GRICULTURE; FORESTRY; ANIMA...\n",
       "1     A21           HUMAN NECESSITIES. BAKING; EDIBLE DOUGHS\n",
       "2     A22  HUMAN NECESSITIES. BUTCHERING; MEAT TREATMENT;...\n",
       "3     A23  HUMAN NECESSITIES. FOODS OR FOODSTUFFS; TREATM...\n",
       "4     A24  HUMAN NECESSITIES. TOBACCO; CIGARS; CIGARETTES..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe with the cpc texts\n",
    "cpc_df = pd.DataFrame(cpc_texts.items(), columns=[\"context\", \"context_text\"])\n",
    "cpc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the dataframe to a csv file\n",
    "cpc_df.to_csv(path / \"cpc_titles.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['context_text'] = train_df['context'].map(cpc_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A47',\n",
       " 'HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; COFFEE MILLS; SPICE MILLS; SUCTION CLEANERS IN GENERAL')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_row = train_df.iloc[0]\n",
    "sample_row.context, sample_row.context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, sep_token):\n",
    "    df[\"section\"] = df.context.str[0]\n",
    "    df[\"sectok\"] = \"[\" + df.section + \"]\"\n",
    "    sectoks = list(df.sectok.unique())\n",
    "    df[\"input\"] = (\n",
    "        df.sectok\n",
    "        + df.anchor.str.lower()\n",
    "        + sep_token\n",
    "        + df.target\n",
    "        + sep_token\n",
    "        + df.context_text.str.lower()\n",
    "    )\n",
    "    return df, sectoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy, _ = process_df(train_df, \"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[A]abatement[SEP]abatement of pollution[SEP]human necessities. furniture; domestic articles or appliances; coffee mills; spice mills; suction cleaners in general'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.iloc[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find a way to fix the upload issue\n",
    "# see: https://github.com/wandb/wandb/issues/4441#issuecomment-1504120929\n",
    "class WandBLoggerNoUpload(WandBLogger):\n",
    "    def can_upload_files(self) -> bool:\n",
    "        \"\"\"Whether the logger supports uploading files.\"\"\"\n",
    "        return False\n",
    "\n",
    "def prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs, run_name):\n",
    "    trainer = Trainer(\n",
    "        model=composer_model,\n",
    "        train_dataloader=train_dl,\n",
    "        eval_dataloader=val_dl,\n",
    "        max_duration=f\"{epochs}ep\",\n",
    "        optimizers=optimizer,\n",
    "        schedulers=[scheduler],\n",
    "        loggers=[WandBLoggerNoUpload(project=\"patent-phrase-to-phrase\")],\n",
    "        run_name=run_name,\n",
    "        device=\"gpu\",\n",
    "        precision=\"amp_fp16\",\n",
    "        step_schedulers_every_batch=True,\n",
    "        # seed=17,\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589a872a296345b584507fd18ca40143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjimmiemunyi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/eleven/code/personal/patent-phrase-to-phrase/wandb/run-20230622_214921-4sgubf32</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/4sgubf32' target=\"_blank\">deberta-v3-large</a></strong> to <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/4sgubf32' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/4sgubf32</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "node_name: ubuntu_desktop\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 975671368\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb30e3785b054cee93210729d96a4eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/1810 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bce9d9d7b847de91dc55d72460755d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   0:    0%|| 0/470 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4942c5f28d4348eeb57d3303580a3bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   1:    0%|| 0/1810 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998502a89e5d46519004a2d359351a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   1:    0%|| 0/470 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d2e2dffaeb49859e575116a6597ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   2:    0%|| 0/1810 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea62eb516a0a42a9a72148e940ed0693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   2:    0%|| 0/470 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6dde33d80c4dc392f3cb4ca412bac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   3:    0%|| 0/1810 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad525b06a27743a58e2aa65b4f6fac65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   3:    0%|| 0/470 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MeanSquaredError': tensor(0.0195, device='cuda:0'), 'PearsonCorrCoef': tensor(0.8523, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"microsoft/deberta-v3-large\"\n",
    "trainer = train(train_df, checkpoint, run_name=\"deberta-v3-large\", sep_token=\"[SEP]\", bs=16, lr=2e-5)\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td></td></tr><tr><td>metrics/eval/MeanSquaredError</td><td></td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td></td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td></td></tr><tr><td>time/batch</td><td></td></tr><tr><td>time/batch_in_epoch</td><td></td></tr><tr><td>time/epoch</td><td></td></tr><tr><td>time/sample</td><td></td></tr><tr><td>time/sample_in_epoch</td><td></td></tr><tr><td>time/token</td><td></td></tr><tr><td>time/token_in_epoch</td><td></td></tr><tr><td>trainer/device_train_microbatch_size</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>0.01367</td></tr><tr><td>metrics/eval/MeanSquaredError</td><td>0.02265</td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td>0.82778</td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td>0.90681</td></tr><tr><td>time/batch</td><td>3620</td></tr><tr><td>time/batch_in_epoch</td><td>0</td></tr><tr><td>time/epoch</td><td>4</td></tr><tr><td>time/sample</td><td>115828</td></tr><tr><td>time/sample_in_epoch</td><td>0</td></tr><tr><td>time/token</td><td>8453327</td></tr><tr><td>time/token_in_epoch</td><td>2111744</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>32</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cpc_texts_sectock</strong> at: <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/b7x0ehjv' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/b7x0ehjv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230620_064858-b7x0ehjv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look on how to make this work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "cpc_df = pd.read_csv(path / \"cpc_titles.csv\")\n",
    "\n",
    "train_df = train_df.merge(cpc_df, left_on=\"context\", right_on=\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>context_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score   \n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  \\\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75   \n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25   \n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50   \n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00   \n",
       "\n",
       "                                        context_text  \n",
       "0  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  \n",
       "1  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  \n",
       "2  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  \n",
       "3  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  \n",
       "4  HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLE...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, sep_token):\n",
    "    df[\"section\"] = df.context.str[0]\n",
    "    df[\"sectok\"] = \"[\" + df.section + \"]\"\n",
    "    sectoks = list(df.sectok.unique())\n",
    "    df[\"input\"] = (\n",
    "        df.sectok\n",
    "        + df.anchor.str.lower()\n",
    "        + sep_token\n",
    "        + df.target\n",
    "        + sep_token\n",
    "        + df.context_text.str.lower()\n",
    "    )\n",
    "    return df, sectoks\n",
    "\n",
    "# TODO: Find a way to fix the upload issue\n",
    "# see: https://github.com/wandb/wandb/issues/4441#issuecomment-1504120929\n",
    "class WandBLoggerNoUpload(WandBLogger):\n",
    "    def can_upload_files(self) -> bool:\n",
    "        \"\"\"Whether the logger supports uploading files.\"\"\"\n",
    "        return False\n",
    "\n",
    "def prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs, run_name):\n",
    "    trainer = Trainer(\n",
    "        model=composer_model,\n",
    "        train_dataloader=train_dl,\n",
    "        eval_dataloader=val_dl,\n",
    "        max_duration=f\"{epochs}ep\",\n",
    "        optimizers=optimizer,\n",
    "        schedulers=[scheduler],\n",
    "        loggers=[WandBLoggerNoUpload(project=\"patent-phrase-to-phrase\")],\n",
    "        run_name=run_name,\n",
    "        device=\"gpu\",\n",
    "        precision=\"amp_fp16\",\n",
    "        step_schedulers_every_batch=True,\n",
    "        # seed=17,\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "from composer.models import ComposerModel\n",
    "\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from typing import Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(ComposerModel):\n",
    "    def __init__(self, checkpoint, tokenizer, config_path=None, pretrained=True, num_labels=1, fc_dropout=0.2):\n",
    "        super().__init__()\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(checkpoint, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            # add tokenizer resizing here\n",
    "            self.model = AutoModel.from_pretrained(checkpoint, config=self.config)\n",
    "            self.model.resize_token_embeddings(len(tokenizer))\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.train_metrics = PearsonCorrCoef(num_outputs=num_labels)\n",
    "        self.val_metrics = MetricCollection([MeanSquaredError(), PearsonCorrCoef(num_outputs=num_labels)])\n",
    "        self._init_weights(self.fc)\n",
    "        self._init_weights(self.attention)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        # feature = torch.mean(last_hidden_states, 1)\n",
    "        weights = self.attention(last_hidden_states)\n",
    "        feature = torch.sum(weights * last_hidden_states, dim=1)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        inputs = {k: v for k, v in batch.items() if k != 'labels'}\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output\n",
    "    \n",
    "    def loss(self, output, batch):\n",
    "        labels = {k: v for k, v in batch.items() if k == 'labels'}\n",
    "        return F.mse_loss(output, labels['labels'].unsqueeze(1))\n",
    "    \n",
    "    def eval_forward(self, batch, outputs: Optional[Any] = None):\n",
    "        if outputs is not None:\n",
    "            return outputs\n",
    "        # inputs = {k: v for k, v in batch.items() if k != 'labels'}\n",
    "        output = self.forward(batch)\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, is_train: bool = False):\n",
    "        if is_train:\n",
    "            metrics = self.train_metrics\n",
    "        else:\n",
    "            metrics = self.val_metrics\n",
    "        \n",
    "        if isinstance(metrics, Metric):\n",
    "            metrics_dict = {metrics.__class__.__name__: metrics}\n",
    "        else:\n",
    "            metrics_dict = {}\n",
    "            for name, metric in metrics.items():\n",
    "                assert isinstance(metric, Metric)\n",
    "                metrics_dict[name] = metric\n",
    "\n",
    "        return metrics_dict\n",
    "    \n",
    "    def update_metric(self, batch, outputs, metric):\n",
    "        labels = {k: v for k, v in batch.items() if k == 'labels'}\n",
    "        metric.update(outputs.squeeze(1), labels['labels'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264ab46d1f8541899833467c60044012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "******************************\n",
      "Config:\n",
      "node_name: ubuntu_desktop\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 3189598527\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b67b6be8154b7dbac6c365043470e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4bc5e4348c4341b925d864d6527dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   0:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a159aa719847c28d37da0ca3d43da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   1:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411f1122ac6643bdb742db61b6dac84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   1:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921908428a2f4abebb5c585d839e04f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   2:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86584420271a41d3b8fd48b044f07bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   2:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8db052555e4559a81dbbefe58ea990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   3:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e0e92d651a4e8590e3d111fa000fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   3:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MeanSquaredError': tensor(0.0273, device='cuda:0'), 'PearsonCorrCoef': tensor(0.8019, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "lr, wd, epochs = 8e-5, 0.01, 4\n",
    "bs, num_labels = 32, 1\n",
    "run_name = \"custom_model\"\n",
    "\n",
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "train_dl, val_dl = prepare_data(train_df, tokenizer, sep_token=\"[SEP]\", bs=bs)\n",
    "model = CustomModel(checkpoint, tokenizer, pretrained=True, num_labels=1)\n",
    "optimizer, scheduler = prepare_optimizer_and_scheduler(model, lr, wd, epochs, train_dl)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dl,\n",
    "    eval_dataloader=val_dl,\n",
    "    max_duration=f\"{epochs}ep\",\n",
    "    optimizers=optimizer,\n",
    "    schedulers=[scheduler],\n",
    "    # loggers=[WandBLoggerNoUpload(project=\"patent-phrase-to-phrase\")],\n",
    "    run_name=run_name,\n",
    "    device=\"gpu\",\n",
    "    precision=\"amp_fp16\",\n",
    "    step_schedulers_every_batch=True,\n",
    ")\n",
    "trainer.fit()\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss/train/total': 0.005640398245304823}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.total_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td></td></tr><tr><td>metrics/eval/MeanSquaredError</td><td></td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td></td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td></td></tr><tr><td>time/batch</td><td></td></tr><tr><td>time/batch_in_epoch</td><td></td></tr><tr><td>time/epoch</td><td></td></tr><tr><td>time/sample</td><td></td></tr><tr><td>time/sample_in_epoch</td><td></td></tr><tr><td>time/token</td><td></td></tr><tr><td>time/token_in_epoch</td><td></td></tr><tr><td>trainer/device_train_microbatch_size</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>0.00564</td></tr><tr><td>metrics/eval/MeanSquaredError</td><td>0.02713</td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td>0.79818</td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td>0.9815</td></tr><tr><td>time/batch</td><td>3624</td></tr><tr><td>time/batch_in_epoch</td><td>0</td></tr><tr><td>time/epoch</td><td>4</td></tr><tr><td>time/sample</td><td>115868</td></tr><tr><td>time/sample_in_epoch</td><td>0</td></tr><tr><td>time/token</td><td>8457853</td></tr><tr><td>time/token_in_epoch</td><td>2114080</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>32</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">custom_model</strong> at: <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/51enrnbm' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/51enrnbm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230622_161749-51enrnbm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h) AWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from composer.models import ComposerModel\n",
    "from composer.core import Algorithm, Event\n",
    "\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from typing import Any, Optional\n",
    "\n",
    "from transformers import AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _restore(\n",
    "        model: Module,\n",
    "        backup: dict) -> None:\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in backup:\n",
    "            param.data = backup[name]\n",
    "    \n",
    "def _save(\n",
    "        model: Module,\n",
    "        adv_param: str,\n",
    "        adv_eps: float,\n",
    "        backup: dict,\n",
    "        backup_eps: dict) -> None:\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None and adv_param in name:\n",
    "            if name not in backup:\n",
    "                backup[name] = param.data.clone()\n",
    "                grad_eps = adv_eps * param.abs().detach()\n",
    "                backup_eps[name] = (\n",
    "                    backup[name] - grad_eps,\n",
    "                    backup[name] + grad_eps,\n",
    "                )\n",
    "\n",
    "def _attack_step(\n",
    "        model: Module,\n",
    "        adv_param: str,\n",
    "        adv_lr: float,\n",
    "        backup_eps: dict) -> None:\n",
    "    e = 1e-6\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None and adv_param in name:\n",
    "            norm1 = torch.norm(param.grad)\n",
    "            norm2 = torch.norm(param.data.detach())\n",
    "            if norm1 != 0 and not torch.isnan(norm1):\n",
    "                r_at = adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                param.data.add_(r_at)\n",
    "                param.data = torch.min(\n",
    "                    torch.max(\n",
    "                        param.data, backup_eps[name][0]), backup_eps[name][1]\n",
    "                )\n",
    "\n",
    "def _attack_backward(\n",
    "        model: Module,\n",
    "        optimizer: Optimizer,\n",
    "        batch, # the current state.batch\n",
    "        adv_param: str,\n",
    "        adv_lr: float,\n",
    "        adv_eps: float,\n",
    "        backup: dict,\n",
    "        backup_eps: dict,\n",
    "        apex: bool) -> Tensor:\n",
    "    with torch.cuda.amp.autocast(enabled=apex):\n",
    "        _save(\n",
    "            model,\n",
    "            adv_param,\n",
    "            adv_eps,\n",
    "            backup,\n",
    "            backup_eps,\n",
    "        )\n",
    "        _attack_step(\n",
    "            model,\n",
    "            adv_param,\n",
    "            adv_lr,\n",
    "            backup_eps,\n",
    "        )\n",
    "        inputs = {k: v for k, v in batch.items() if k != 'labels'}\n",
    "        labels = {k: v for k, v in batch.items() if k == 'labels'}['labels']\n",
    "        y_preds = model(inputs)\n",
    "        adv_loss = model.loss(\n",
    "            y_preds, batch)\n",
    "        mask = (labels.view(-1, 1) != -1)\n",
    "        adv_loss = torch.masked_select(adv_loss, mask).mean()\n",
    "        optimizer.zero_grad()\n",
    "    return adv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP(Algorithm):\n",
    "    def __init__(self, \n",
    "                 start_epoch: int, \n",
    "                 adv_param: str = 'weight',\n",
    "                 adv_lr: float = 1.0,\n",
    "                 adv_eps: float = 0.01,\n",
    "                 apex: bool = True):\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.apex = apex\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "    \n",
    "    def match(self, event, state):\n",
    "        return event == Event.AFTER_BACKWARD and state.timestamp.epoch >= self.start_epoch\n",
    "    \n",
    "    def apply(self, event, state, logger):\n",
    "        state.loss = _attack_backward(\n",
    "            state.model, \n",
    "            state.optimizers[0], \n",
    "            state.batch,\n",
    "            self.adv_param,\n",
    "            self.adv_lr,\n",
    "            self.adv_eps,\n",
    "            self.backup,\n",
    "            self.backup_eps,\n",
    "            self.apex)\n",
    "        state.scaler.scale(state.loss).backward()\n",
    "        # state.loss.backward()\n",
    "        _restore(state.model, self.backup)\n",
    "        self.backup, self.backup_eps = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(ComposerModel):\n",
    "    def __init__(self, checkpoint, tokenizer, config_path=None, pretrained=True, num_labels=1, fc_dropout=0.2):\n",
    "        super().__init__()\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(checkpoint, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            # add tokenizer resizing here\n",
    "            self.model = AutoModel.from_pretrained(checkpoint, config=self.config)\n",
    "            self.model.resize_token_embeddings(len(tokenizer))\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.train_metrics = PearsonCorrCoef(num_outputs=num_labels)\n",
    "        self.val_metrics = MetricCollection([MeanSquaredError(), PearsonCorrCoef(num_outputs=num_labels)])\n",
    "        self._init_weights(self.fc)\n",
    "        self._init_weights(self.attention)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        # feature = torch.mean(last_hidden_states, 1)\n",
    "        weights = self.attention(last_hidden_states)\n",
    "        feature = torch.sum(weights * last_hidden_states, dim=1)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        inputs = {k: v for k, v in batch.items() if k != 'labels'}\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output\n",
    "    \n",
    "    def loss(self, output, batch):\n",
    "        labels = {k: v for k, v in batch.items() if k == 'labels'}\n",
    "        return F.mse_loss(output, labels['labels'].unsqueeze(1))\n",
    "    \n",
    "    def eval_forward(self, batch, outputs: Optional[Any] = None):\n",
    "        if outputs is not None:\n",
    "            return outputs\n",
    "        # inputs = {k: v for k, v in batch.items() if k != 'labels'}\n",
    "        output = self.forward(batch)\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, is_train: bool = False):\n",
    "        if is_train:\n",
    "            metrics = self.train_metrics\n",
    "        else:\n",
    "            metrics = self.val_metrics\n",
    "        \n",
    "        if isinstance(metrics, Metric):\n",
    "            metrics_dict = {metrics.__class__.__name__: metrics}\n",
    "        else:\n",
    "            metrics_dict = {}\n",
    "            for name, metric in metrics.items():\n",
    "                assert isinstance(metric, Metric)\n",
    "                metrics_dict[name] = metric\n",
    "\n",
    "        return metrics_dict\n",
    "    \n",
    "    def update_metric(self, batch, outputs, metric):\n",
    "        labels = {k: v for k, v in batch.items() if k == 'labels'}\n",
    "        metric.update(outputs.squeeze(1), labels['labels'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, sep_token):\n",
    "    df[\"section\"] = df.context.str[0]\n",
    "    df[\"sectok\"] = \"[\" + df.section + \"]\"\n",
    "    sectoks = list(df.sectok.unique())\n",
    "    df[\"input\"] = (\n",
    "        df.sectok\n",
    "        + df.anchor.str.lower()\n",
    "        + sep_token\n",
    "        + df.target\n",
    "        + sep_token\n",
    "        + df.context_text.str.lower()\n",
    "    )\n",
    "    return df, sectoks\n",
    "\n",
    "# TODO: Find a way to fix the upload issue\n",
    "# see: https://github.com/wandb/wandb/issues/4441#issuecomment-1504120929\n",
    "class WandBLoggerNoUpload(WandBLogger):\n",
    "    def can_upload_files(self) -> bool:\n",
    "        \"\"\"Whether the logger supports uploading files.\"\"\"\n",
    "        return False\n",
    "\n",
    "def prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs, run_name):\n",
    "    trainer = Trainer(\n",
    "        model=composer_model,\n",
    "        train_dataloader=train_dl,\n",
    "        eval_dataloader=val_dl,\n",
    "        max_duration=f\"{epochs}ep\",\n",
    "        optimizers=optimizer,\n",
    "        schedulers=[scheduler],\n",
    "        loggers=[WandBLoggerNoUpload(project=\"patent-phrase-to-phrase\")],\n",
    "        algorithms=[AWP()],\n",
    "        run_name=run_name,\n",
    "        device=\"gpu\",\n",
    "        precision=\"amp_fp16\",\n",
    "        step_schedulers_every_batch=True,\n",
    "        # seed=17,\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "cpc_df = pd.read_csv(path / \"cpc_titles.csv\")\n",
    "\n",
    "train_df = train_df.merge(cpc_df, left_on=\"context\", right_on=\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcecb4032544dc2abd61104f27c9f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjimmiemunyi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/eleven/code/personal/patent-phrase-to-phrase/wandb/run-20230701_151847-ujqxfyjv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/ujqxfyjv' target=\"_blank\">awp</a></strong> to <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/ujqxfyjv' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/ujqxfyjv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "enabled_algorithms/AWP: true\n",
      "node_name: ubuntu_desktop\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 3528865605\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab8400797d64a36b001077753b51655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094198abb85e40dfa9de5c888a9b7239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   0:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c3ccf703e84ba8bd7956aba9999b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   1:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec1194ab0fb427d893d1aa61801a62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   1:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6872e3a7eb4403a02b376e477c3080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   2:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1aabd76066547a0bb4cc54c765aeb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   2:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101ec4f1ab7e4df19d607a82ef4eb2f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   3:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceeaf41a4a3c43deb36feb67c2cdb2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   3:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd2d8ee524643ba8c098587f0644698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   4:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf389468bba743b6906c158da71568c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   4:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MeanSquaredError': tensor(0.0256, device='cuda:0'), 'PearsonCorrCoef': tensor(0.8055, device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr, wd, epochs = 2e-5, 0.01, 5\n",
    "bs, num_labels = 32, 1\n",
    "run_name = \"awp\"\n",
    "\n",
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "train_dl, val_dl = prepare_data(train_df, tokenizer, sep_token=\"[SEP]\", bs=bs)\n",
    "model = CustomModel(checkpoint, tokenizer, pretrained=True, num_labels=1)\n",
    "optimizer, scheduler = prepare_optimizer_and_scheduler(model, lr, wd, epochs, train_dl)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dl,\n",
    "    eval_dataloader=val_dl,\n",
    "    max_duration=f\"{epochs}ep\",\n",
    "    optimizers=optimizer,\n",
    "    schedulers=[scheduler],\n",
    "    loggers=[WandBLoggerNoUpload(project=\"patent-phrase-to-phrase\")],\n",
    "    algorithms=[AWP(start_epoch=1, adv_lr=1e-4, adv_eps=1e-2)],\n",
    "    run_name=run_name,\n",
    "    device=\"gpu\",\n",
    "    precision=\"amp_fp16\",\n",
    "    step_schedulers_every_batch=True,\n",
    ")\n",
    "trainer.fit()\n",
    "print(trainer.state.eval_metric_values)\n",
    "trainer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
