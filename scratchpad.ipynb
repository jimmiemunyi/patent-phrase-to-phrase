{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import colorlog\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from fastcore.xtras import Path  # for ls\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchmetrics import PearsonCorrCoef, MeanSquaredError\n",
    "from composer.models.huggingface import HuggingFaceModel\n",
    "from composer.loggers import WandBLogger\n",
    "from composer import Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignoring warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def process_df(df, sep_token):\n",
    "    df[\"section\"] = df.context.str[0]\n",
    "    df[\"sectok\"] = \"[\" + df.section + \"]\"\n",
    "    sectoks = list(df.sectok.unique())\n",
    "    df[\"input\"] = (\n",
    "        df.sectok\n",
    "        + sep_token\n",
    "        + df.context\n",
    "        + sep_token\n",
    "        + df.anchor.str.lower()\n",
    "        + sep_token\n",
    "        + df.target\n",
    "    )\n",
    "    \n",
    "    return df, sectoks\n",
    "\n",
    "\n",
    "def create_val_split(df: pd.DataFrame, val_prop: float = 0.2, seed: int = 42):\n",
    "    anchors = df.anchor.unique()\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(anchors)\n",
    "    val_sz = int(len(anchors) * val_prop)\n",
    "    val_anchors = anchors[:val_sz]\n",
    "    is_val = np.isin(df.anchor, val_anchors)\n",
    "    idxs = np.arange(len(df))\n",
    "    val_idxs = idxs[is_val]\n",
    "    trn_idxs = idxs[~is_val]\n",
    "\n",
    "    return trn_idxs, val_idxs\n",
    "\n",
    "def tokenize_func(batch, tokenizer):\n",
    "    return tokenizer(\n",
    "        batch[\"input\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize_and_split(df, tokenize_func, train=True):\n",
    "    inps = \"anchor\", \"target\", \"context\"\n",
    "    dataset = datasets.Dataset.from_pandas(df)\n",
    "    tok_dataset = dataset.map(\n",
    "        tokenize_func,\n",
    "        batched=True,\n",
    "        batch_size=None,\n",
    "        remove_columns=inps + (\"id\", \"input\", \"section\", \"sectok\")\n",
    "    )\n",
    "    if train:\n",
    "        tok_dataset = tok_dataset.rename_columns({\"score\": \"labels\"})\n",
    "        trn_idxs, val_idxs = create_val_split(df)\n",
    "        tok_dataset = datasets.DatasetDict(\n",
    "        {\"train\": tok_dataset.select(trn_idxs), \"test\": tok_dataset.select(val_idxs)}\n",
    "    )\n",
    "    \n",
    "    return tok_dataset\n",
    "\n",
    "\n",
    "def create_dataloaders(tok_ds, bs, train=True):\n",
    "    if train:\n",
    "        train_dl = DataLoader(\n",
    "            tok_ds[\"train\"],\n",
    "            batch_size=bs,\n",
    "            shuffle=True,\n",
    "            collate_fn=default_data_collator,\n",
    "        )\n",
    "        val_dl = DataLoader(\n",
    "            tok_ds[\"test\"],\n",
    "            batch_size=bs,\n",
    "            shuffle=False,\n",
    "            collate_fn=default_data_collator,\n",
    "        )\n",
    "\n",
    "        return train_dl, val_dl\n",
    "    else:\n",
    "        test_dl = DataLoader(\n",
    "            tok_ds,\n",
    "            batch_size=bs,\n",
    "            shuffle=False,\n",
    "            collate_fn=default_data_collator,\n",
    "        )\n",
    "\n",
    "        return test_dl\n",
    "\n",
    "\n",
    "def predict(trainer, test_dl):\n",
    "    preds = trainer.predict(test_dl)[0][\"logits\"].numpy().astype(float)\n",
    "    preds = np.clip(preds, 0, 1)\n",
    "    preds = preds.round(2)\n",
    "    preds = preds.squeeze()\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df, tokenizer, sep_token, bs):\n",
    "    train_df, sectoks = process_df(train_df, sep_token)\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": sectoks})\n",
    "    tokenize = partial(tokenize_func, tokenizer=tokenizer)\n",
    "    train_tok_ds = tokenize_and_split(train_df, tokenize)\n",
    "    train_dl, val_dl = create_dataloaders(train_tok_ds, bs)\n",
    "    \n",
    "    return train_dl, val_dl\n",
    "\n",
    "def prepare_model(checkpoint, num_labels, tokenizer):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint, num_labels=num_labels\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    pears_corr = PearsonCorrCoef(num_outputs=num_labels)\n",
    "    mse_metric = MeanSquaredError()\n",
    "    composer_model = HuggingFaceModel(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        metrics=[pears_corr],\n",
    "        eval_metrics=[mse_metric, pears_corr],\n",
    "        use_logits=True,\n",
    "    )\n",
    "    \n",
    "    return composer_model\n",
    "\n",
    "def prepare_optimizer_and_scheduler(composer_model, lr, wd, epochs, train_dl):\n",
    "    optimizer = AdamW(\n",
    "        params=composer_model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.98),\n",
    "        eps=1e-6,\n",
    "        weight_decay=wd,\n",
    "    )\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        steps_per_epoch=len(train_dl),\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler\n",
    "\n",
    "def prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs, run_name):\n",
    "    trainer = Trainer(\n",
    "        model=composer_model,\n",
    "        train_dataloader=train_dl,\n",
    "        eval_dataloader=val_dl,\n",
    "        max_duration=f\"{epochs}ep\",\n",
    "        optimizers=optimizer,\n",
    "        schedulers=[scheduler],\n",
    "        loggers=[WandBLogger(project=\"patent-phrase-to-phrase\")],\n",
    "        run_name=run_name,\n",
    "        device=\"gpu\",\n",
    "        precision=\"amp_fp16\",\n",
    "        step_schedulers_every_batch=True,\n",
    "        # seed=17,\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_df, checkpoint, run_name, bs=32, lr=8e-5, wd=0.01, epochs=4, num_labels=1, sep_token=\" [s] \"):\n",
    "    # preparing data\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    train_dl, val_dl = prepare_data(train_df, tokenizer, sep_token, bs)\n",
    "    \n",
    "    # preparing model\n",
    "    composer_model = prepare_model(checkpoint, num_labels, tokenizer)\n",
    "    \n",
    "    # preparing optimizer and scheduler\n",
    "    optimizer, scheduler = prepare_optimizer_and_scheduler(composer_model, lr, wd, epochs, train_dl)\n",
    "    \n",
    "    # preparing trainer\n",
    "    trainer = prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs, run_name)\n",
    "    \n",
    "    # training\n",
    "    trainer.fit()\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "test_df = pd.read_csv(path / \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token = \" [s] \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, sectoks = process_df(train_df, sep_token)\n",
    "eval_df, _ = process_df(test_df, sep_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/deberta-v3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens({\"additional_special_tokens\": sectoks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = partial(tokenize_func, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok_ds = tokenize_and_split(train_df, tokenize)\n",
    "eval_tok_ds = tokenize_and_split(eval_df, tokenize, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5\n",
    "bs = 64\n",
    "epochs = 4\n",
    "num_labels =1\n",
    "wd = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl = create_dataloaders(train_tok_ds, bs)\n",
    "test_dl = create_dataloaders(eval_tok_ds, bs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample batch and print the first element\n",
    "print(\"Sample batch\")\n",
    "batch = next(iter(val_dl))\n",
    "print(batch[\"input_ids\"][0])\n",
    "print(batch[\"token_type_ids\"][0])\n",
    "print(batch[\"attention_mask\"][0])\n",
    "print(batch[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"token_type_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pears_corr = PearsonCorrCoef(num_outputs=num_labels)\n",
    "mse_metric = MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_model = HuggingFaceModel(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    metrics=[pears_corr],\n",
    "    eval_metrics=[mse_metric, pears_corr],\n",
    "    use_logits=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    params=composer_model.parameters(),\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-6,\n",
    "    weight_decay=wd,\n",
    ")\n",
    "\n",
    "one_cycle_lr = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=lr,\n",
    "    steps_per_epoch=len(train_dl),\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "trainer = train(train_df, checkpoint, run_name=\"baseline\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation Zone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Different Sep Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5\n",
    "bs = 64\n",
    "epochs = 4\n",
    "num_labels =1\n",
    "wd = 0.01\n",
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "# sep_token = \" [s] \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "train_dl, val_dl = prepare_data(train_df, tokenizer, sep_token=tokenizer.sep_token, bs=bs)\n",
    "composer_model = prepare_model(checkpoint, num_labels, tokenizer)\n",
    "optimizer, scheduler = prepare_optimizer_and_scheduler(composer_model, lr, wd, epochs, train_dl)\n",
    "trainer = prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs, run_name=\"tok_sep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.eval_metric_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Classification task instead of Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_to_class = {\n",
    "    0: 0,\n",
    "    0.25: 1,\n",
    "    0.5: 2,\n",
    "    0.75: 3,\n",
    "    1: 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the function to the dataframe\n",
    "train_df[\"score\"] = train_df[\"score\"].apply(lambda x: score_to_class[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5\n",
    "bs = 64\n",
    "epochs = 4\n",
    "num_labels = 5\n",
    "wd = 0.01\n",
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "sep_token = \" [s] \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "from composer.metrics import CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model_classification(checkpoint, num_labels, tokenizer):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint, num_labels=num_labels\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    pears_corr = PearsonCorrCoef(num_outputs=num_labels)\n",
    "    cross_entropy = CrossEntropy()\n",
    "    accuracy_metric = Accuracy(task='multiclass', num_classes=num_labels)\n",
    "    composer_model = HuggingFaceModel(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        metrics=[cross_entropy, accuracy_metric],\n",
    "        # eval_metrics=[mse_metric, pears_corr],\n",
    "        use_logits=True,\n",
    "    )\n",
    "    \n",
    "    return composer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "train_dl, val_dl = prepare_data(train_df, tokenizer, sep_token=sep_token, bs=bs)\n",
    "composer_model = prepare_model_classification(checkpoint, num_labels, tokenizer)\n",
    "optimizer, scheduler = prepare_optimizer_and_scheduler(composer_model, lr, wd, epochs, train_dl)\n",
    "trainer = prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.eval_metric_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could not get the main metric (Pearson Correlation) to work in a classification setting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Different Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "checkpoint = \"anferico/bert-for-patents\"\n",
    "trainer = train(train_df, checkpoint, lr=8e-6, run_name=\"bert-for-patents\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "checkpoint = \"AI-Growth-Lab/PatentSBERTa\"\n",
    "trainer = train(train_df, checkpoint, run_name=\"PatentSBERTa\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Cosine Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from composer.optim import CosineAnnealingWithWarmupScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_optimizer_and_scheduler(composer_model, lr, wd, epochs, train_dl):\n",
    "    optimizer = AdamW(\n",
    "        params=composer_model.parameters(),\n",
    "        lr=lr,\n",
    "        betas=(0.9, 0.98),\n",
    "        eps=1e-6,\n",
    "        weight_decay=wd,\n",
    "    )\n",
    "    scheduler = CosineAnnealingWithWarmupScheduler(\n",
    "        t_warmup='0.2dur'\n",
    "    )\n",
    "    \n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "trainer = train(train_df, checkpoint, run_name=\"cosine_scheduler\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Replacing the Context with the explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score\n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75\n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25\n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50\n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               37d61fd2272659b1\n",
       "anchor                  abatement\n",
       "target     abatement of pollution\n",
       "context                       A47\n",
       "score                         0.5\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_row = train_df.iloc[0]\n",
    "sample_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_csv(path / \"titles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>group</th>\n",
       "      <th>main_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>HUMAN NECESSITIES</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01</td>\n",
       "      <td>AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTI...</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01B</td>\n",
       "      <td>SOIL WORKING IN AGRICULTURE OR FORESTRY; PARTS...</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01B1/00</td>\n",
       "      <td>Hand tools (edge trimmers for lawns A01G3/06  ...</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01B1/02</td>\n",
       "      <td>Spades; Shovels {(hand-operated dredgers E02F3...</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>B</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       code                                              title section  class   \n",
       "0         A                                  HUMAN NECESSITIES       A    NaN  \\\n",
       "1       A01  AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTI...       A    1.0   \n",
       "2      A01B  SOIL WORKING IN AGRICULTURE OR FORESTRY; PARTS...       A    1.0   \n",
       "3  A01B1/00  Hand tools (edge trimmers for lawns A01G3/06  ...       A    1.0   \n",
       "4  A01B1/02  Spades; Shovels {(hand-operated dredgers E02F3...       A    1.0   \n",
       "\n",
       "  subclass  group  main_group  \n",
       "0      NaN    NaN         NaN  \n",
       "1      NaN    NaN         NaN  \n",
       "2        B    NaN         NaN  \n",
       "3        B    1.0         0.0  \n",
       "4        B    1.0         2.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; COFFEE MILLS; SPICE MILLS; SUCTION CLEANERS IN GENERAL'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles.loc[titles[\"code\"] == sample_row[\"context\"]].title.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes matching the context to the code\n",
    "train_df = train_df.merge(titles, left_on=\"context\", right_on=\"code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>code</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>group</th>\n",
       "      <th>main_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score code   \n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  A47  \\\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  A47   \n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25  A47   \n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50  A47   \n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00  A47   \n",
       "\n",
       "                                               title section  class subclass   \n",
       "0  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN  \\\n",
       "1  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "2  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "3  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "4  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "\n",
       "   group  main_group  \n",
       "0    NaN         NaN  \n",
       "1    NaN         NaN  \n",
       "2    NaN         NaN  \n",
       "3    NaN         NaN  \n",
       "4    NaN         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the title instead of the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, sep_token):\n",
    "    df[\"section\"] = df.context.str[0]\n",
    "    df[\"sectok\"] = \"[\" + df.section + \"]\"\n",
    "    sectoks = list(df.sectok.unique())\n",
    "    df[\"input\"] = (\n",
    "        df.sectok\n",
    "        + sep_token\n",
    "        # + \"context: \"\n",
    "        + df.title.str.lower()\n",
    "        + sep_token\n",
    "        + df.anchor.str.lower()\n",
    "        + sep_token\n",
    "        + df.target\n",
    "    )\n",
    "    \n",
    "    return df, sectoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy, _ = process_df(train_df, \" [s] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[A] [s] furniture; domestic articles or appliances; coffee mills; spice mills; suction cleaners in general [s] abatement [s] abatement of pollution'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.iloc[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[A] [s] furniture; domestic articles or appliances; coffee mills; spice mills; suction cleaners in general [s] cervical support [s] gel pack'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.iloc[100].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>code</th>\n",
       "      <th>title</th>\n",
       "      <th>section</th>\n",
       "      <th>class</th>\n",
       "      <th>subclass</th>\n",
       "      <th>group</th>\n",
       "      <th>main_group</th>\n",
       "      <th>sectok</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[A] [s] furniture; domestic articles or applia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[A] [s] furniture; domestic articles or applia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[A] [s] furniture; domestic articles or applia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[A] [s] furniture; domestic articles or applia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>A47</td>\n",
       "      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n",
       "      <td>A</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[A] [s] furniture; domestic articles or applia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score code   \n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50  A47  \\\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75  A47   \n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25  A47   \n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50  A47   \n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00  A47   \n",
       "\n",
       "                                               title section  class subclass   \n",
       "0  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN  \\\n",
       "1  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "2  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "3  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "4  FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...       A   47.0      NaN   \n",
       "\n",
       "   group  main_group sectok                                              input  \n",
       "0    NaN         NaN    [A]  [A] [s] furniture; domestic articles or applia...  \n",
       "1    NaN         NaN    [A]  [A] [s] furniture; domestic articles or applia...  \n",
       "2    NaN         NaN    [A]  [A] [s] furniture; domestic articles or applia...  \n",
       "3    NaN         NaN    [A]  [A] [s] furniture; domestic articles or applia...  \n",
       "4    NaN         NaN    [A]  [A] [s] furniture; domestic articles or applia...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd99f3cd1994b3481a2553bfbc061ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjimmiemunyi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/eleven/code/personal/patent-phrase-to-phrase/wandb/run-20230620_044435-z0jxe85e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/z0jxe85e' target=\"_blank\">context_title</a></strong> to <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/z0jxe85e' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/z0jxe85e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "node_name: ubuntu_desktop\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 456369393\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8872476a6f42738714edc28bb2a905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1bca121d354b10933b1705da99dfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   0:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a1ea769c2640e793e977ef7de46adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   1:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dafbbceca314f0285472deec5667ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   1:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19c6169cb064f17b8e521e9d0e355e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   2:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a554befbc0d74518b93aeab6c5478929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   2:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842842eaf36a48d78d18f7ad8c99bdba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   3:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2017ecfb7e8340cca016d18e1bd8ce7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   3:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MeanSquaredError': tensor(0.0268, device='cuda:0'), 'PearsonCorrCoef': tensor(0.7982, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "trainer = train(train_df, checkpoint, run_name=\"context_title\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some improvements! Potential is there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>█▆▆▃▆▃▅▃▄▃▃▃▄▃▃▂▂▂▂▃▂▃▂▂▁▂▂▁▂▂▁▁▁▁▁▂▂▂▁▂</td></tr><tr><td>metrics/eval/MeanSquaredError</td><td>█▃▁▂</td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td>▁▆██</td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td>▃▁▂▆▅▇▆▆▆▇▇▇▆▅▇▇███▇█▆████▇██▇█████████▇</td></tr><tr><td>time/batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/batch_in_epoch</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▃▃▄▅▅▆▇█▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>time/epoch</td><td>▁▃▅▆█</td></tr><tr><td>time/sample</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/sample_in_epoch</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▃▃▄▅▅▆▇█▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>time/token</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/token_in_epoch</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▃▃▄▅▅▆▇█▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>0.00601</td></tr><tr><td>metrics/eval/MeanSquaredError</td><td>0.02683</td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td>0.79822</td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td>0.94622</td></tr><tr><td>time/batch</td><td>3624</td></tr><tr><td>time/batch_in_epoch</td><td>0</td></tr><tr><td>time/epoch</td><td>4</td></tr><tr><td>time/sample</td><td>115868</td></tr><tr><td>time/sample_in_epoch</td><td>0</td></tr><tr><td>time/token</td><td>8689575</td></tr><tr><td>time/token_in_epoch</td><td>2172000</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>32</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">context_title</strong> at: <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/z0jxe85e' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/z0jxe85e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230620_044435-z0jxe85e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Different arrangements of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, sep_token):\n",
    "    df[\"section\"] = df.context.str[0]\n",
    "    df[\"sectok\"] = \"[\" + df.section + \"]\"\n",
    "    sectoks = list(df.sectok.unique())\n",
    "    df[\"input\"] = (\n",
    "        df.anchor.str.lower()\n",
    "        + sep_token\n",
    "        + df.target\n",
    "        + sep_token\n",
    "        + df.title.str.lower()\n",
    "    )\n",
    "    # df[\"input\"] = (\n",
    "    #     df.sectok\n",
    "    #     + sep_token\n",
    "    #     # + \"context: \"\n",
    "    #     + df.title.str.lower()\n",
    "    #     + sep_token\n",
    "    #     + df.anchor.str.lower()\n",
    "    #     + sep_token\n",
    "    #     + df.target\n",
    "    # )\n",
    "    \n",
    "    return df, sectoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "titles = pd.read_csv(path / \"titles.csv\")\n",
    "train_df = train_df.merge(titles, left_on=\"context\", right_on=\"code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy, _ = process_df(train_df, \" [s] \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abatement [s] abatement of pollution [s] furniture; domestic articles or appliances; coffee mills; spice mills; suction cleaners in general'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.iloc[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e46ec97ec0440895bbde3ddb03546c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/eleven/code/personal/patent-phrase-to-phrase/wandb/run-20230620_051446-i4w4z8iy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/i4w4z8iy' target=\"_blank\">anchor_target_title</a></strong> to <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/i4w4z8iy' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/i4w4z8iy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "node_name: ubuntu_desktop\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 2979262394\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c33f8365654ba0af37530a0d9b5cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0962eba891424ef9badf96b282fc9413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   0:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845f837818624230be0fa224ed4bdd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   1:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0179039bc77d4b86b1bd9285304635b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   1:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933a1f3d9d9f4330ba37c6d02333f268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   2:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c08976aaf049cebb9283a1e8eb534d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   2:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e9e0e8d6934d00bfa858d2347b0a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   3:    0%|| 0/906 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1e445dd84d438db2a04487a4cdc52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   3:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MeanSquaredError': tensor(0.0273, device='cuda:0'), 'PearsonCorrCoef': tensor(0.7924, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "trainer = train(train_df, checkpoint, run_name=\"anchor_target_title\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>█▃▄▃▂▃▂▂▂▂▁▁▂▁▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>metrics/eval/MeanSquaredError</td><td>██▁▃</td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td>▁▄██</td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td>▁▃▄▄▅▄▆▇▇▇▇▇▇█▇▇▇▇▇█▇███▇███▇▇████▇█████</td></tr><tr><td>time/batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/batch_in_epoch</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▃▃▄▅▅▆▇█▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>time/epoch</td><td>▁▃▅▆█</td></tr><tr><td>time/sample</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/sample_in_epoch</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▃▃▄▅▅▆▇█▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>time/token</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/token_in_epoch</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▃▃▄▅▅▆▇█▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>0.02186</td></tr><tr><td>metrics/eval/MeanSquaredError</td><td>0.02733</td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td>0.79245</td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td>0.81018</td></tr><tr><td>time/batch</td><td>3624</td></tr><tr><td>time/batch_in_epoch</td><td>0</td></tr><tr><td>time/epoch</td><td>4</td></tr><tr><td>time/sample</td><td>115868</td></tr><tr><td>time/sample_in_epoch</td><td>0</td></tr><tr><td>time/token</td><td>8226131</td></tr><tr><td>time/token_in_epoch</td><td>2056160</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>32</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">anchor_target_title</strong> at: <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/i4w4z8iy' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/i4w4z8iy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230620_051446-i4w4z8iy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g) Updates from Kaggle 1st Place Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPC Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path / \"train.csv\")\n",
    "cpc_path = Path('cpc-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('cpc-data/CPCSchemeXML202105'),Path('cpc-data/CPCTitleList202202')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpc_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_cpc_texts():\n",
    "    contexts = []\n",
    "    pattern = '[A-Z]\\d+'\n",
    "    for file_name in (cpc_path/'CPCSchemeXML202105').ls():\n",
    "        result = re.findall(pattern, file_name.name)\n",
    "        if result:\n",
    "            contexts.append(result)\n",
    "    contexts = sorted(set(sum(contexts, [])))\n",
    "    results = {}\n",
    "    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n",
    "        with open(cpc_path/f'CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n",
    "            s = f.read()\n",
    "        pattern = f'{cpc}\\t\\t.+'\n",
    "        result = re.findall(pattern, s)\n",
    "        cpc_result = result[0].lstrip(pattern)\n",
    "        if cpc == 'C':\n",
    "            cpc_result = 'C' + cpc_result\n",
    "        for context in [c for c in contexts if c[0] == cpc]:\n",
    "            pattern = f'{context}\\t\\t.+'\n",
    "            result = re.findall(pattern, s)\n",
    "            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpc_texts = get_cpc_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['context_text'] = train_df['context'].map(cpc_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A47',\n",
       " 'HUMAN NECESSITIES. FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; COFFEE MILLS; SPICE MILLS; SUCTION CLEANERS IN GENERAL')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_row = train_df.iloc[0]\n",
    "sample_row.context, sample_row.context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, sep_token):\n",
    "    df[\"section\"] = df.context.str[0]\n",
    "    df[\"sectok\"] = \"[\" + df.section + \"]\"\n",
    "    sectoks = list(df.sectok.unique())\n",
    "    df[\"input\"] = (\n",
    "        df.sectok\n",
    "        + df.anchor.str.lower()\n",
    "        + sep_token\n",
    "        + df.target\n",
    "        + sep_token\n",
    "        + df.context_text.str.lower()\n",
    "    )\n",
    "    return df, sectoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy, _ = process_df(train_df, \"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[A]abatement[SEP]abatement of pollution[SEP]human necessities. furniture; domestic articles or appliances; coffee mills; spice mills; suction cleaners in general'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.iloc[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find a way to fix the upload issue\n",
    "# see: https://github.com/wandb/wandb/issues/4441#issuecomment-1504120929\n",
    "class WandBLoggerNoUpload(WandBLogger):\n",
    "    def can_upload_files(self) -> bool:\n",
    "        \"\"\"Whether the logger supports uploading files.\"\"\"\n",
    "        return False\n",
    "\n",
    "def prepare_trainer(composer_model, optimizer, scheduler, train_dl, val_dl, epochs, run_name):\n",
    "    trainer = Trainer(\n",
    "        model=composer_model,\n",
    "        train_dataloader=train_dl,\n",
    "        eval_dataloader=val_dl,\n",
    "        max_duration=f\"{epochs}ep\",\n",
    "        optimizers=optimizer,\n",
    "        schedulers=[scheduler],\n",
    "        loggers=[WandBLoggerNoUpload(project=\"patent-phrase-to-phrase\")],\n",
    "        run_name=run_name,\n",
    "        device=\"gpu\",\n",
    "        precision=\"amp_fp16\",\n",
    "        step_schedulers_every_batch=True,\n",
    "        # seed=17,\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6917e12e750469885f52a7ca1a96bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7368df53f6524987bd60d44b1cccaf0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01667060941666326, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/eleven/code/personal/patent-phrase-to-phrase/wandb/run-20230620_064858-b7x0ehjv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/b7x0ehjv' target=\"_blank\">cpc_texts_sectock</a></strong> to <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/b7x0ehjv' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/b7x0ehjv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "node_name: ubuntu_desktop\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 3298515164\n",
      "\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0baada2736454daa6c06d9bea4dd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   0:    0%|| 0/905 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfbb5ad52924a3e9fdd2e609540ff9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   0:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225f31c578bf4825af2feaef3394b306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   1:    0%|| 0/905 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc80de5738c242ba837f303ea5445f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   1:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245e2f4922b04532ba16158423f7a0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   2:    0%|| 0/905 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35984fcf69bc4396bd8ef2d5dc440cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   2:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c269b478d674feba7a944f645c26b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train          Epoch   3:    0%|| 0/905 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5e73230f564f8797127521e2f41e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval           Epoch   3:    0%|| 0/235 [00:00<?, ?ba/s]         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MeanSquaredError': tensor(0.0227, device='cuda:0'), 'PearsonCorrCoef': tensor(0.8278, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "trainer = train(train_df, checkpoint, run_name=\"cpc_texts_sectock\", sep_token=\"[SEP]\")\n",
    "\n",
    "print(trainer.state.eval_metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>▇██▆▆▃▃▃▃▅▅▅▃▂▂▃▂▂▄▃▃▂▂▂▃▂▂▂▂▂▁▁▁▁▂▁▂▂▂▁</td></tr><tr><td>metrics/eval/MeanSquaredError</td><td>█▃▁▁</td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td>▁▆██</td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td>▂▁▂▅▃▆▇▆▅▅▆▆█▇█▇▇▇▅▆▇██▆▆▇███████▇████▇█</td></tr><tr><td>time/batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/batch_in_epoch</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▃▃▄▅▅▆▇█▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>time/epoch</td><td>▁▃▅▆█</td></tr><tr><td>time/sample</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/sample_in_epoch</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▃▃▄▅▅▆▇█▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>time/token</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>time/token_in_epoch</td><td>▁▂▂▃▄▅▅▆▇▇▁▂▃▃▄▅▅▆▇█▁▂▃▃▄▅▆▆▇█▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss/train/total</td><td>0.01367</td></tr><tr><td>metrics/eval/MeanSquaredError</td><td>0.02265</td></tr><tr><td>metrics/eval/PearsonCorrCoef</td><td>0.82778</td></tr><tr><td>metrics/train/PearsonCorrCoef</td><td>0.90681</td></tr><tr><td>time/batch</td><td>3620</td></tr><tr><td>time/batch_in_epoch</td><td>0</td></tr><tr><td>time/epoch</td><td>4</td></tr><tr><td>time/sample</td><td>115828</td></tr><tr><td>time/sample_in_epoch</td><td>0</td></tr><tr><td>time/token</td><td>8453327</td></tr><tr><td>time/token_in_epoch</td><td>2111744</td></tr><tr><td>trainer/device_train_microbatch_size</td><td>32</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cpc_texts_sectock</strong> at: <a href='https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/b7x0ehjv' target=\"_blank\">https://wandb.ai/jimmiemunyi/patent-phrase-to-phrase/runs/b7x0ehjv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230620_064858-b7x0ehjv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, checkpoint, config_path=None, pretrained=True, num_labels=1, fc_dropout=0.2):\n",
    "        super().__init__()\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(checkpoint, output_hidden_states=True)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(checkpoint, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel.from_config(self.config)\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        self._init_weights(self.fc)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.config.hidden_size, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self._init_weights(self.attention)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        # feature = torch.mean(last_hidden_states, 1)\n",
    "        weights = self.attention(last_hidden_states)\n",
    "        feature = torch.sum(weights * last_hidden_states, dim=1)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(self.fc_dropout(feature))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"microsoft/deberta-v3-small\"\n",
    "model = CustomModel(checkpoint, pretrained=True, fc_dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
